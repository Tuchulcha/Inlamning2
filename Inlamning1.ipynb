{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inlämning 1\n",
    "Jag tänker ofta på svenska, även i text. Har försökt börja använda engelska för att underlätta användandet av AI verktyg men jag är inte riktigt där än. \n",
    "Här är repon: https://github.com/Tuchulcha/Inlamning1\n",
    "git och jag är inte vänner än, så den kanske inte är updaterad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "file_path = \"AmazonDataSales.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False. Interesting\n",
    "#Using low_memory=False to get more accuracy in inference of datatypes. Increases the odds of pandas choosing the right datatype somehow?\n",
    "df = pd.read_csv(file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "num_columns = df.shape[1]\n",
    "#pd.set_option('display.max_info_columns', num_columns)\n",
    "df.info(num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out more about columns with mixed data types\n",
    "mixed_type_columns = []\n",
    "for i, col in enumerate(df.columns):\n",
    "    types = {type(v) for v in df[col]}\n",
    "    if len(types) > 1:\n",
    "        mixed_type_columns.append((i, col))\n",
    "        print(f\"Data types in column {i} {col}:\", types)\n",
    "\n",
    "print(f\"{len(mixed_type_columns)} Columns with mixed types\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistical overview of numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating and displaying the number of missing values in each column \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "# Handle missing values and data inconsistencies\n",
    "Replace missing values in 'Courier Status', 'currency', 'Amount', 'ship-city', 'ship-state', 'ship-postal-code', and 'ship-country' with appropriate values or methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing 'Courier Status' with \"Unknown\"\n",
    "# df['Courier Status'].fillna('Unknown', inplace=True) Seems like inplace=True is being disbanded\n",
    "df['Courier Status'] = df['Courier Status'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous row fill missing 'currency'\n",
    "df['currency'] = df['currency'].ffill(axis = 0)\n",
    "print(len(df[df['currency'].apply(lambda x: isinstance(x, float))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'Amount' with mean\n",
    "df['Amount'] = df['Amount'].fillna(value=df['Amount'].mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore 'ship-city', 'ship-state', 'ship-postal-code', and 'ship-country'\n",
    "#Maybe city and state isn't missing when country is, for example.\n",
    "# Filter the DataFrame to only include rows with missing values in the specified columns\n",
    "missing_values_df = df[df[['ship-city', 'ship-state', 'ship-postal-code', 'ship-country']].isna().any(axis=1)]\n",
    "\n",
    "missing_values_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are misisng for all four columns at the same time, so no possibility of infering country from state and city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in missing values in 'ship-city', 'ship-state', 'ship-postal-code', and 'ship-country' with \"Missing\"\n",
    "df[['ship-city', 'ship-state', 'ship-postal-code', 'ship-country']] = df[['ship-city', 'ship-state', 'ship-postal-code', 'ship-country']].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with a high percentage of missing values if they are not crucial\n",
    "df.drop(columns=['fulfilled-by', 'Unnamed: 22', 'promotion-ids'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "## Check for and resolve any data inconsistencies, such as incorrect data types or unusual values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes look ok\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No negative values in 'Qty' or 'Amount'\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with mixed data types again\n",
    "mixed_type_columns = []\n",
    "for i, col in enumerate(df.columns):\n",
    "    types = {type(v) for v in df[col]}\n",
    "    if len(types) > 1:\n",
    "        mixed_type_columns.append((i, col))\n",
    "        print(f\"Data types in column {i} {col}:\", types)\n",
    "\n",
    "print(f\"{len(mixed_type_columns)} Columns with mixed types\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "currency? First time I ran this code it was only 'ship-postal-code' that was mixed type. Not this column. Maybe I just missed it but I will go back and check.\n",
    "I had missed df['currency'] = or inplace= operator\n",
    "before df['currency'].ffill(axis = 0)\n",
    "It's fixed now but I'll leave this in to show that it was a process. Also inplace= got \"decrapiated\" while I was doing this, maybe my laptop which i proof read on got a newer version of pandas installed? \n",
    "The below code was to explore if something was off with 'currency' but I think I just missed the inplace operator or df['currency'] = before df['currency'].ffill(axis = 0)\n",
    "As mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring 'currency'\n",
    "# Filter the DataFrame to include only rows where the value in 'currency' is a float\n",
    "float_values_df = df[df['currency'].apply(lambda x: isinstance(x, float))]\n",
    "\n",
    "# Count and display the occurrences of each unique float value\n",
    "float_value_counts = float_values_df['currency'].value_counts()\n",
    "print(float_value_counts)\n",
    "\n",
    "print(len(df[df['currency'].apply(lambda x: isinstance(x, float))]))\n",
    "\n",
    "# Count NaN values in the column\n",
    "nan_count = df['currency'].isna().sum()\n",
    "print(f\"Number of NaN values in 'currency': {nan_count}\")\n",
    "\n",
    "# Count non-NaN float values in the column\n",
    "non_nan_floats = df['currency'].apply(lambda x: isinstance(x, float) and not pd.isna(x)).sum()\n",
    "print(f\"Number of non-NaN float values in 'currency': {non_nan_floats}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at df.ddtypes it's clear that all object type columns should be string datatype except 'Date'. \n",
    "# The float types in the Data types in column 18 'ship-postal-code' will be converted to string when the dtype of the column is changed from object to string.\n",
    "\n",
    "# Convert all object type columns to string type\n",
    "df = df.astype({col: 'string' for col in df.select_dtypes('object').columns})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I wonder what compute cost needlessly converting the 'Date' column carries.\n",
    "I had this thought that it takes unecessary compute to convert the 'Date' column to a string type when it will be converted later to date type. This is correct. But how I went about trying it was wrong. Of course the right comparison is not between converting all columns with one line of code and making a loop checking if every column is the 'Date' column and converting only if it isn't.\n",
    "The right comparison would be to convert all the columns with one command vs making an explicit list with all the columns to be converted (this list containing all columns except the 'Date' column). The caveat being that this is not 'future' proof. Maybe a more salient comparison is making a list using code, and removing 'Date' from this list with code, to account for if the number of columns changes? Hmmm. Maybe it's not easy to say what the right comparison is...\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df = df.astype({col: 'string' for col in df.select_dtypes('object').columns})\n",
    "end_time = time.time()\n",
    "time_with_date_conversion = end_time - start_time\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "exclude_columns = ['Date']\n",
    "df = df.astype({col: 'string' for col in df.select_dtypes('object').columns if col not in exclude_columns})\n",
    "end_time = time.time()\n",
    "time_without_date_conversion = end_time - start_time\n",
    "\n",
    "\n",
    "print(f\"Time with date conversion: {time_with_date_conversion} seconds\")\n",
    "print(f\"Time without date conversion: {time_without_date_conversion} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date problems\n",
    "Okay, something is not right with the 'Date' data. Manually looking at the data i find dates such as \"2004-01-22\" and \"05-31-22\". The only way the latter date works is if it's in the format \"MM-DD-YY\". Looking at the data Index 72469 to 72470 goes from 05-13-22 to 2005-12-22. Index 90997 to 90998 goes from 2005-01-22 to 04-30-22. It seems like there's a mix of formats and I find the jump in years from 2022 to 2005 and then from 2005 to 2022 to be odd.\n",
    "\n",
    "Since my manual sampling shows dates that can only be YYYY-MM-DD and dates that can only be MM-DD-YY. If I change YYYY-MM-DD to MM-DD-YY the dates will be congruent given that my observation holds for the data. One way to do this could be to check the length of the value in each row in the column.\n",
    "\n",
    "pandas seems to support more advanced checks like checking the current format of the date per row and doing some kind of conversion\n",
    "\n",
    "So a single line of code was enough to do the conversion: df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y', errors='coerce') \n",
    "Leaving this block in here as a memento of the hours spent prompting trying to find some other solution\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%m-%d-%y', errors='coerce')\n",
    "\n",
    "# Count the number of NaT values in the 'Date' column\n",
    "nat_count = df['Date'].isna().sum()\n",
    "print(f\"Number of NaT values in 'Date' column: {nat_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check datatypes again\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize text data for uniformity (e.g., Category, Size, Style)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all text to lowercase for uniformity\n",
    "df['Category'] = df['Category'].str.lower()\n",
    "# Or capitalize the first letter of each word\n",
    "#df['Category'] = df['Category'].str.title()\n",
    "# is it for machine learning that all lower is better? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add one for 'Size' and 'Style' as well\n",
    "print(f\"Size strings that were entirely uppercase: {len(df['Size'].str.isupper())} out of {len(df['Size'])}\")\n",
    "print(f\"Style strings that were entirely uppercase: {len(df['Style'].str.isupper())} out of {len(df['Style'])}\")\n",
    "# Same here, is it for machine learning that all lower is better? It's already consistent and it looks\n",
    "# better the way that it is now. But sure!\n",
    "df['Size'] = df['Size'].str.lower()\n",
    "df['Style'] = df['Style'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets assume that all string values in all columns are supposed to be lower case\n",
    "for col in df.select_dtypes(include=['string']):\n",
    "    df[col] = df[col].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing spaces\n",
    "\n",
    "#First check if there are any leading or trailing spaces in string acolumns\n",
    "columns_with_trailspace = 0\n",
    "columns_with_leadspace = 0\n",
    "for col in df.select_dtypes(include=['string']):\n",
    "    trailing_spaces = df[col].str.endswith(' ')\n",
    "    leading_spaces = df[col].str.startswith(' ')\n",
    "    if trailing_spaces.any():\n",
    "        print(f\"Column '{col}' has trailing spaces.\")\n",
    "        columns_with_trailspace += 1\n",
    "    if leading_spaces.any():\n",
    "        print(f\"Column '{col}' has leading spaces.\")\n",
    "        columns_with_leadspace += 1\n",
    "print(f\"amount of columns with rows that have trailing spaces: {columns_with_trailspace}\")\n",
    "print(f\"amount of columns with rows that have leading spaces: {columns_with_leadspace}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing spaces\n",
    "df['ship-state'] = df['ship-state'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jag kanske går ån efter vatten här? Är det värt att kolla efter whitespace att trimma? Det kanske är mer compute effektivt att bara trimma direkt, utan att veta om det finns något att trimma?\n",
    "\n",
    "Att kolla efter space att trimma går igenom all data en gång, att trimma allt går igenom data en gång. Så... kanske bara bättre att trimma direkt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading and trailing whitespace from all column names\n",
    "df.columns = df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace any inconsistencies or typos, if known\n",
    "What columns could be candidates?\n",
    "Status, Fulfilment, Sales Channel, ship-service-level, Category, Courier Status, ship-city,ship-state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to apply value_counts\n",
    "columns_to_check = ['Status', 'Fulfilment', 'Sales Channel', \n",
    "                    'ship-service-level', 'Category', 'Size', 'Courier Status', \n",
    "                    'ship-city', 'ship-state']\n",
    "\n",
    "# Loop through each column and print the value counts\n",
    "for col in columns_to_check:\n",
    "    try:\n",
    "        print(f\"Value counts for '{col}':\")\n",
    "        print(df[col].value_counts())\n",
    "    except KeyError:\n",
    "        print(f\"Column '{col}' not found in DataFrame.\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 'Sales Channel' column separately because it didn't work\n",
    "try:\n",
    "    print(\"Value counts for 'Sales Channel':\")\n",
    "    print(df['Sales Channel'].value_counts())\n",
    "except KeyError as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "# Don't know why it doesn't work, but it looks right manually checking it.\n",
    "# Ah! figured it out, whitespace in the column name! I will correct it higher up in the code \n",
    "# With the other leading and trailing whitespace removal. Leaving this for posterity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'ship-city' and 'ship-state' has obvious typos. Ship-city a lot more than ship-state.\n",
    "\n",
    "tail_value_counts = df['ship-city'].value_counts().tail(15)\n",
    "print(tail_value_counts)\n",
    "\n",
    "tail_value_counts = df['ship-state'].value_counts().tail(15)\n",
    "print(tail_value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many are there?\n",
    "sample_ship_cities = df['ship-city'].unique()\n",
    "print(len(sample_ship_cities))  \n",
    "#Yeah... that's too much for my scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ship_state = df['ship-state'].unique()\n",
    "print(len(sample_ship_state))\n",
    "#This is manageable\n",
    "sample_ship_state[:100]  # Displaying the first 100 unique state names as a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to fix some of them. Let's check the tail for good candidates\n",
    "tail_value_counts = df['ship-state'].value_counts().tail(48)\n",
    "print(tail_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajsthan, rj, rajshthan Corrects to: rajasthan\n",
    "# pondicherry Corrects to: puducherry\n",
    "# orissa Corrects to: odisha\n",
    "# I'm not sure what the corrections are for the rest so let's settle for these.\n",
    "# Create a mapping dictionary for the corrections\n",
    "corrections = {\n",
    "    'rajsthan': 'rajasthan',\n",
    "    'rj': 'rajasthan',\n",
    "    'rajshthan': 'rajasthan',\n",
    "    'pondicherry': 'puducherry',\n",
    "    'orissa': 'odisha'\n",
    "}\n",
    "\n",
    "# Apply the corrections to the 'ship-state' column\n",
    "df['ship-state'] = df['ship-state'].replace(corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets assume that it's a good thing to standardize column headers as well!\n",
    "# Let's go with lowercase all the way.\n",
    "df.columns = df.columns.str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the dataset after standardizing\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the dataset after standardizing\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck the dataset after standardizing\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we've done some exploration, looked for inconsistencies and missing values, filled in missing values. Specified data-type for columns, made all string values lower-case, as well as the column headers. Fixed some spelling error.\n",
    "Left in some of the process instead of making it stream-lined for austerity since this is for my education, not production.\n",
    "\n",
    "I think this ends the DataProcessMLSemi.py part. According to the pdf Data_PreProcessing_Exercise this is left:\n",
    "3- Feature engineering\n",
    "4- Data Transformation\n",
    "5- Presentation and Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make month and year columns\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a b2b binary column\n",
    "df['b2b_binary'] = df['b2b'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group 'amount' by interval\n",
    "# This makes four bins with the same number of rows being labeled with each\n",
    "# Use pd.qcut() to create the 'Label' column based on the distribution of 'Value'\n",
    "df['amount_quartiles_bin'] = pd.qcut(df['amount'], q=4, labels=['0-25', '25-50', '50-75', '75-100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure there aren't a boatload of 0 values\n",
    "#print(df.nsmallest(20, 'amount'))\n",
    "min_value = df['amount'].min()\n",
    "print((df['amount'] == min_value).sum())\n",
    "# This is fine, maybe, I guesstimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the max value isn't a complete outlier\n",
    "n = 100  # For example, the top 10 largest values\n",
    "\n",
    "# Display the n largest values in the 'Amount' column\n",
    "largest_amounts = df['amount'].nlargest(n)\n",
    "print(largest_amounts)\n",
    "# This is fine, maybe, I guesstimate.\n",
    "# Say that it wasn't fine and the outliers skews the data df['amount_Log'] = np.log1p(df['amount']) would be a way to fix it. You could say the difference between the size of \n",
    "# numbers gets smaller and smaller as the size of the difference increases if you \"log\" it\n",
    "# or you could just \"do one dirty\" and chop off the top 3 values from the generation of the normalized column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm interested in making labels where the labels are min to max divided in to four labels\n",
    "# max for 'amount' is 5584. So 0-25, 25-50, 50-75, 75-100 is:\n",
    "# 0-1396, 1397-2792, 2793-4188, 4189-5584\n",
    "\n",
    "# Define custom bin edges\n",
    "custom_bins = [0, 1396, 2792, 4188, 5584]\n",
    "\n",
    "# Define labels for the bins\n",
    "labels = ['0-25', '25-50', '50-75', '75-100']\n",
    "\n",
    "# Use pd.cut() to create the 'Label' column based on custom bins\n",
    "df['amount_relative_range_bins'] = pd.cut(df['amount'], bins=custom_bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize 'qty' and 'amount'\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Normalization to 'qty' and 'amount'\n",
    "df[['qty_minmax_norm', 'amount_minmax_norm']] = scaler.fit_transform(df[['qty', 'amount']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: somehow I messed up using git and github. I was mostly done, just doing some polish but was going away for the weekend so I made a repo for working on my laptop. My guess is that I didn't save in vscode on my desktop before committing and pushing. Then I pulled the file on my laptop, did some changes without scrolling to the end of the file, making sure it was correct. Committed and pushed, came home, pulled and in the process lost what I had done before I went away. But I don't know. git and github makes me confused. Anyway. Lost the work I'd done on one-hot encoding and describing what I had done. I still have the csv file from what I did. It's called \"AmazonCleanedAndSpiced.csv\" and is in my repo. I guess \"I messed up saving my file and pushing\" is my version of \"the dog ate my homework\"\n",
    "I'll power through and remake it with a bit less care as it's sunday, late, and I'm pooped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts about categorical columns\n",
    "Some columns that could be considered categorical have high cardinality so I'll skip them because of size.\n",
    "sku and asin are both high cardinality and probably identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode categorical columns with one-hot encoding\n",
    "categorical_columns = ['Status', \n",
    "                       'Fulfilment', \n",
    "                       'Sales Channel', \n",
    "                       'ship-service-level', \n",
    "                       'Category', \n",
    "                       'Courier Status' \n",
    "                       #'ship-city', \n",
    "                       #'ship-state', \n",
    "                       #'ship-country', \n",
    "                       #'Style'\n",
    "                       ]\n",
    "\n",
    "# Apply one-hot encoding with a prefix\n",
    "for col in categorical_columns:\n",
    "    dummies = pd.get_dummies(df[col], prefix=f\"o-he_{col}\")\n",
    "    df = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('AmazonCleanedAndQuickSpiced.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this the data has been looked over, missing values filled in, some spelling errors have been sorted, a few derivated columns have been created. Normalized columns have been created as well as one-hot encoded for a few categorical columns. Data and columns names have been changed to all lower-case.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBIA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

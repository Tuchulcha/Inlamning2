{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tono\\AppData\\Local\\Temp\\ipykernel_21924\\2486837581.py:2: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('AmazonDataSales_v2.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not normalized score R2/MSE:(0.42399884819387323, 187.29478076027766)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('AmazonDataSales_v2.csv')\n",
    "# Drop all columns except 'amount', 'category', 'size', 'quantity'\n",
    "df = df[['amount', 'category', 'size', 'qty']]\n",
    "\n",
    "# One-hot encode the 'category', 'size', and 'qty' columns\n",
    "# Select all columns except 'amount' as feature columns\n",
    "feature_columns = df.columns.drop('amount')\n",
    "# One-hot encode the feature columns\n",
    "df_encoded = pd.get_dummies(df, columns=feature_columns)\n",
    "\n",
    "# Assuming 'df' contains your dataset\n",
    "X = df_encoded.drop('amount', axis=1)  # Features\n",
    "y = df['amount']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate r2 score and rsme\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    \n",
    "    return r2, rmse\n",
    "\n",
    "#define model\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "#train model\n",
    "regressor_train = train_evaluate_model(regressor, X_train, y_train, X_test, y_test)\n",
    "print(f\"Not normalized score R2/MSE:{regressor_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature columns: 25\n"
     ]
    }
   ],
   "source": [
    "#Count the number of feature columns\n",
    "num_feature_columns = len(df_encoded.columns)\n",
    "print(f\"Number of feature columns: {num_feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays (required for PyTorch tensors)\n",
    "X_np = np.array(X, dtype=np.float32)\n",
    "y_np = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_np)\n",
    "y_tensor = torch.tensor(y_np)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/15, Loss: 350616.90808886057\n",
      "Epoch 2/15, Loss: 113674.98082615859\n",
      "Epoch 3/15, Loss: 42775.57843191965\n",
      "Epoch 4/15, Loss: 35087.43879211841\n",
      "Epoch 5/15, Loss: 34943.26577580251\n",
      "Epoch 6/15, Loss: 34843.96644145939\n",
      "Epoch 7/15, Loss: 34816.967092501065\n",
      "Epoch 8/15, Loss: 34808.88633211097\n",
      "Epoch 9/15, Loss: 34753.171328922195\n",
      "Epoch 10/15, Loss: 34762.42282498937\n",
      "Epoch 11/15, Loss: 34735.12168168049\n",
      "Epoch 12/15, Loss: 34756.59190648916\n",
      "Epoch 13/15, Loss: 34762.411862909226\n",
      "Epoch 14/15, Loss: 34752.0398344494\n",
      "Epoch 15/15, Loss: 34730.5900131537\n",
      "Test Loss: 35202.059713612434\n",
      "R-squared: 0.4217062677253687\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transformer Model for Regression\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, num_transformer_blocks, output_dim):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input for transformer: [sequence_length, batch_size, feature_size]\n",
    "        x = x.transpose(0, 1)  # Swap batch_size and sequence_length dimensions\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = x.transpose(0, 1)  # Swap back the dimensions\n",
    "        return self.linear(x[:, 0, :]).view(-1, 1)  # Reshape output to [batch_size, 1]\n",
    "\n",
    "# Model instantiation\n",
    "model = TransformerRegressor(\n",
    "    input_dim=X_train.shape[1], \n",
    "    num_heads=1, \n",
    "    ff_dim=64, \n",
    "    num_transformer_blocks=1, \n",
    "    output_dim=1\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)   \n",
    "            #          \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)  # Ensure targets are the correct shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)  # Ensure targets are the correct shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())  # Move targets back to CPU\n",
    "            outputs_list.append(outputs.cpu())  # Move outputs back to CPU\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')\n",
    "\n",
    "# Run training and evaluation\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=15)\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2000, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 452110.6028645833\n",
      "Epoch 2/200, Loss: 444347.5670572917\n",
      "Epoch 3/200, Loss: 408235.7604166667\n",
      "Epoch 4/200, Loss: 326383.2757161458\n",
      "Epoch 5/200, Loss: 209532.048828125\n",
      "Epoch 6/200, Loss: 106709.93245442708\n",
      "Epoch 7/200, Loss: 63018.721028645836\n",
      "Epoch 8/200, Loss: 52489.43107096354\n",
      "Epoch 9/200, Loss: 48140.10213216146\n",
      "Epoch 10/200, Loss: 44876.283854166664\n",
      "Epoch 11/200, Loss: 42639.10986328125\n",
      "Epoch 12/200, Loss: 40100.312744140625\n",
      "Epoch 13/200, Loss: 38843.72550455729\n",
      "Epoch 14/200, Loss: 37697.788248697914\n",
      "Epoch 15/200, Loss: 37085.298177083336\n",
      "Epoch 16/200, Loss: 36236.6875\n",
      "Epoch 17/200, Loss: 35820.05887858073\n",
      "Epoch 18/200, Loss: 35432.542724609375\n",
      "Epoch 19/200, Loss: 35300.19132486979\n",
      "Epoch 20/200, Loss: 34755.94685872396\n",
      "Epoch 21/200, Loss: 34835.57702636719\n",
      "Epoch 22/200, Loss: 35079.6083984375\n",
      "Epoch 23/200, Loss: 34798.88814290365\n",
      "Epoch 24/200, Loss: 34713.94384765625\n",
      "Epoch 25/200, Loss: 34490.59338378906\n",
      "Epoch 26/200, Loss: 35264.5009358724\n",
      "Epoch 27/200, Loss: 34576.10489908854\n",
      "Epoch 28/200, Loss: 34666.38289388021\n",
      "Epoch 29/200, Loss: 34555.82364908854\n",
      "Epoch 30/200, Loss: 34833.36275227865\n",
      "Epoch 31/200, Loss: 34488.7919921875\n",
      "Epoch 32/200, Loss: 34500.28633626302\n",
      "Epoch 33/200, Loss: 34573.218017578125\n",
      "Epoch 34/200, Loss: 34315.05423990885\n",
      "Epoch 35/200, Loss: 34912.12170410156\n",
      "Epoch 36/200, Loss: 34666.82661946615\n",
      "Epoch 37/200, Loss: 34483.06160481771\n",
      "Epoch 38/200, Loss: 34623.08260091146\n",
      "Epoch 39/200, Loss: 34562.05183919271\n",
      "Epoch 40/200, Loss: 34895.864013671875\n",
      "Epoch 41/200, Loss: 35123.52364095052\n",
      "Epoch 42/200, Loss: 34494.81571451823\n",
      "Epoch 43/200, Loss: 34333.15580240885\n",
      "Epoch 44/200, Loss: 34509.131673177086\n",
      "Epoch 45/200, Loss: 34333.75675455729\n",
      "Epoch 46/200, Loss: 34321.29744466146\n",
      "Epoch 47/200, Loss: 34275.34029134115\n",
      "Epoch 48/200, Loss: 34427.65474446615\n",
      "Epoch 49/200, Loss: 34835.88049316406\n",
      "Epoch 50/200, Loss: 34347.54463704427\n",
      "Epoch 51/200, Loss: 34447.2157796224\n",
      "Epoch 52/200, Loss: 34629.81453450521\n",
      "Epoch 53/200, Loss: 34363.60201009115\n",
      "Epoch 54/200, Loss: 34418.20593261719\n",
      "Epoch 55/200, Loss: 34514.63370768229\n",
      "Epoch 56/200, Loss: 34862.92126464844\n",
      "Epoch 57/200, Loss: 34398.66963704427\n",
      "Epoch 58/200, Loss: 34514.990478515625\n",
      "Epoch 59/200, Loss: 34728.95756022135\n",
      "Epoch 60/200, Loss: 34487.542643229164\n",
      "Epoch 61/200, Loss: 34518.508544921875\n",
      "Epoch 62/200, Loss: 34448.94901529948\n",
      "Epoch 63/200, Loss: 34569.42297363281\n",
      "Epoch 64/200, Loss: 34452.863932291664\n",
      "Epoch 65/200, Loss: 34373.9150390625\n",
      "Epoch 66/200, Loss: 34514.884440104164\n",
      "Epoch 67/200, Loss: 34466.412109375\n",
      "Epoch 68/200, Loss: 34437.380208333336\n",
      "Epoch 69/200, Loss: 34601.02596028646\n",
      "Epoch 70/200, Loss: 34412.730712890625\n",
      "Epoch 71/200, Loss: 34665.085611979164\n",
      "Epoch 72/200, Loss: 34757.136393229164\n",
      "Epoch 73/200, Loss: 34482.64383951823\n",
      "Epoch 74/200, Loss: 34804.86669921875\n",
      "Epoch 75/200, Loss: 34333.18729654948\n",
      "Epoch 76/200, Loss: 34752.1698811849\n",
      "Epoch 77/200, Loss: 34276.11576334635\n",
      "Epoch 78/200, Loss: 34290.284830729164\n",
      "Epoch 79/200, Loss: 34701.412434895836\n",
      "Epoch 80/200, Loss: 34966.75150553385\n",
      "Epoch 81/200, Loss: 34433.14978027344\n",
      "Epoch 82/200, Loss: 34481.54121907552\n",
      "Epoch 83/200, Loss: 34948.33581542969\n",
      "Epoch 84/200, Loss: 34502.55000813802\n",
      "Epoch 85/200, Loss: 34761.405517578125\n",
      "Epoch 86/200, Loss: 34598.65572102865\n",
      "Epoch 87/200, Loss: 34586.185791015625\n",
      "Epoch 88/200, Loss: 34677.350341796875\n",
      "Epoch 89/200, Loss: 34559.66906738281\n",
      "Epoch 90/200, Loss: 34600.27746582031\n",
      "Epoch 91/200, Loss: 34901.478190104164\n",
      "Epoch 92/200, Loss: 34406.198486328125\n",
      "Epoch 93/200, Loss: 34544.95137532552\n",
      "Epoch 94/200, Loss: 34425.503743489586\n",
      "Epoch 95/200, Loss: 34373.11055501302\n",
      "Epoch 96/200, Loss: 34452.2949625651\n",
      "Epoch 97/200, Loss: 34740.39562988281\n",
      "Epoch 98/200, Loss: 34349.17598470052\n",
      "Epoch 99/200, Loss: 34544.72082519531\n",
      "Epoch 100/200, Loss: 34449.740397135414\n",
      "Epoch 101/200, Loss: 34499.17370605469\n",
      "Epoch 102/200, Loss: 34607.4882405599\n",
      "Epoch 103/200, Loss: 34427.114990234375\n",
      "Epoch 104/200, Loss: 34866.134602864586\n",
      "Epoch 105/200, Loss: 34894.27754720052\n",
      "Epoch 106/200, Loss: 34504.54988606771\n",
      "Epoch 107/200, Loss: 34554.466959635414\n",
      "Epoch 108/200, Loss: 34859.12687174479\n",
      "Epoch 109/200, Loss: 34754.70918782552\n",
      "Epoch 110/200, Loss: 34445.34606933594\n",
      "Epoch 111/200, Loss: 35002.76127115885\n",
      "Epoch 112/200, Loss: 34939.42032877604\n",
      "Epoch 113/200, Loss: 34599.19250488281\n",
      "Epoch 114/200, Loss: 34509.61657714844\n",
      "Epoch 115/200, Loss: 34775.771809895836\n",
      "Epoch 116/200, Loss: 34631.93123372396\n",
      "Epoch 117/200, Loss: 34569.76477050781\n",
      "Epoch 118/200, Loss: 34359.76127115885\n",
      "Epoch 119/200, Loss: 34186.610371907555\n",
      "Epoch 120/200, Loss: 34460.923014322914\n",
      "Epoch 121/200, Loss: 34497.33459472656\n",
      "Epoch 122/200, Loss: 34182.606201171875\n",
      "Epoch 123/200, Loss: 34207.69942220052\n",
      "Epoch 124/200, Loss: 34557.901529947914\n",
      "Epoch 125/200, Loss: 34486.42411295573\n",
      "Epoch 126/200, Loss: 35040.046630859375\n",
      "Epoch 127/200, Loss: 34893.85245768229\n",
      "Epoch 128/200, Loss: 34680.98234049479\n",
      "Epoch 129/200, Loss: 34252.27071126302\n",
      "Epoch 130/200, Loss: 34284.0390625\n",
      "Epoch 131/200, Loss: 34540.23299153646\n",
      "Epoch 132/200, Loss: 34488.12414550781\n",
      "Epoch 133/200, Loss: 34486.56001790365\n",
      "Epoch 134/200, Loss: 34898.29602050781\n",
      "Epoch 135/200, Loss: 34753.94913736979\n",
      "Epoch 136/200, Loss: 34547.08044433594\n",
      "Epoch 137/200, Loss: 34378.41259765625\n",
      "Epoch 138/200, Loss: 34749.92427571615\n",
      "Epoch 139/200, Loss: 34323.23522949219\n",
      "Epoch 140/200, Loss: 34949.890869140625\n",
      "Epoch 141/200, Loss: 34515.88431803385\n",
      "Epoch 142/200, Loss: 34540.9745686849\n",
      "Epoch 143/200, Loss: 34590.07352701823\n",
      "Epoch 144/200, Loss: 34503.26241048177\n",
      "Epoch 145/200, Loss: 34376.894205729164\n",
      "Epoch 146/200, Loss: 34455.67960611979\n",
      "Epoch 147/200, Loss: 34850.6288655599\n",
      "Epoch 148/200, Loss: 34445.5898844401\n",
      "Epoch 149/200, Loss: 34242.78865559896\n",
      "Epoch 150/200, Loss: 34327.3545328776\n",
      "Epoch 151/200, Loss: 34683.75606282552\n",
      "Epoch 152/200, Loss: 34891.85046386719\n",
      "Epoch 153/200, Loss: 34525.19087727865\n",
      "Epoch 154/200, Loss: 34586.94425455729\n",
      "Epoch 155/200, Loss: 34658.343017578125\n",
      "Epoch 156/200, Loss: 34410.45166015625\n",
      "Epoch 157/200, Loss: 34396.412760416664\n",
      "Epoch 158/200, Loss: 34574.044677734375\n",
      "Epoch 159/200, Loss: 34565.62829589844\n",
      "Epoch 160/200, Loss: 34416.34810384115\n",
      "Epoch 161/200, Loss: 34702.68151855469\n",
      "Epoch 162/200, Loss: 34318.02298990885\n",
      "Epoch 163/200, Loss: 34494.375\n",
      "Epoch 164/200, Loss: 34561.994384765625\n",
      "Epoch 165/200, Loss: 34673.04972330729\n",
      "Epoch 166/200, Loss: 34714.4062906901\n",
      "Epoch 167/200, Loss: 34751.17557779948\n",
      "Epoch 168/200, Loss: 34475.86381022135\n",
      "Epoch 169/200, Loss: 34508.509033203125\n",
      "Epoch 170/200, Loss: 34349.40637207031\n",
      "Epoch 171/200, Loss: 34831.10795084635\n",
      "Epoch 172/200, Loss: 34315.66316731771\n",
      "Epoch 173/200, Loss: 34322.05196126302\n",
      "Epoch 174/200, Loss: 34904.41231282552\n",
      "Epoch 175/200, Loss: 34672.135579427086\n",
      "Epoch 176/200, Loss: 34530.49226888021\n",
      "Epoch 177/200, Loss: 34384.060384114586\n",
      "Epoch 178/200, Loss: 34340.44494628906\n",
      "Epoch 179/200, Loss: 34674.43839518229\n",
      "Epoch 180/200, Loss: 34575.852864583336\n",
      "Epoch 181/200, Loss: 34399.50467936198\n",
      "Epoch 182/200, Loss: 34639.64099121094\n",
      "Epoch 183/200, Loss: 34877.31689453125\n",
      "Epoch 184/200, Loss: 34502.00077311198\n",
      "Epoch 185/200, Loss: 34462.64774576823\n",
      "Epoch 186/200, Loss: 34554.88228352865\n",
      "Epoch 187/200, Loss: 34635.18017578125\n",
      "Epoch 188/200, Loss: 34845.817626953125\n",
      "Epoch 189/200, Loss: 34290.215006510414\n",
      "Epoch 190/200, Loss: 34466.86022949219\n",
      "Epoch 191/200, Loss: 34345.758138020836\n",
      "Epoch 192/200, Loss: 34392.24255371094\n",
      "Epoch 193/200, Loss: 34439.23425292969\n",
      "Epoch 194/200, Loss: 34456.135904947914\n",
      "Epoch 195/200, Loss: 34422.190755208336\n",
      "Epoch 196/200, Loss: 34695.313802083336\n",
      "Epoch 197/200, Loss: 34309.571451822914\n",
      "Epoch 198/200, Loss: 34535.6943766276\n",
      "Epoch 199/200, Loss: 34414.89306640625\n",
      "Epoch 200/200, Loss: 34604.06481933594\n",
      "Test Loss: 35188.675455729164\n",
      "R-squared: 0.42263425495989904\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(FeedForwardRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation and move to device\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 64\n",
    "hidden_size2 = 32\n",
    "model = FeedForwardRegressor(input_size, hidden_size1, hidden_size2).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())\n",
    "            outputs_list.append(outputs.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')\n",
    "\n",
    "# Run training and evaluation\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=200)\n",
    "evaluate_model(model, test_loader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 451025.8834635417\n",
      "Epoch 2/300, Loss: 450513.373046875\n",
      "Epoch 3/300, Loss: 451224.9088541667\n",
      "Epoch 4/300, Loss: 450796.853515625\n",
      "Epoch 5/300, Loss: 449535.9759114583\n",
      "Epoch 6/300, Loss: 447373.68359375\n",
      "Epoch 7/300, Loss: 447136.96484375\n",
      "Epoch 8/300, Loss: 445613.4225260417\n",
      "Epoch 9/300, Loss: 443922.5872395833\n",
      "Epoch 10/300, Loss: 442466.5787760417\n",
      "Epoch 11/300, Loss: 441471.2415364583\n",
      "Epoch 12/300, Loss: 440091.7005208333\n",
      "Epoch 13/300, Loss: 436661.4713541667\n",
      "Epoch 14/300, Loss: 433224.9778645833\n",
      "Epoch 15/300, Loss: 430045.4010416667\n",
      "Epoch 16/300, Loss: 427313.029296875\n",
      "Epoch 17/300, Loss: 423067.2493489583\n",
      "Epoch 18/300, Loss: 420710.1627604167\n",
      "Epoch 19/300, Loss: 417021.3984375\n",
      "Epoch 20/300, Loss: 409546.7369791667\n",
      "Epoch 21/300, Loss: 405269.5266927083\n",
      "Epoch 22/300, Loss: 400449.634765625\n",
      "Epoch 23/300, Loss: 393208.4231770833\n",
      "Epoch 24/300, Loss: 387109.03515625\n",
      "Epoch 25/300, Loss: 380936.3639322917\n",
      "Epoch 26/300, Loss: 374089.7317708333\n",
      "Epoch 27/300, Loss: 368266.6803385417\n",
      "Epoch 28/300, Loss: 360224.322265625\n",
      "Epoch 29/300, Loss: 353065.0983072917\n",
      "Epoch 30/300, Loss: 345195.869140625\n",
      "Epoch 31/300, Loss: 336471.2200520833\n",
      "Epoch 32/300, Loss: 328187.7272135417\n",
      "Epoch 33/300, Loss: 320011.3880208333\n",
      "Epoch 34/300, Loss: 310633.9576822917\n",
      "Epoch 35/300, Loss: 300927.3391927083\n",
      "Epoch 36/300, Loss: 293680.8483072917\n",
      "Epoch 37/300, Loss: 283591.677734375\n",
      "Epoch 38/300, Loss: 274791.3072916667\n",
      "Epoch 39/300, Loss: 264349.0592447917\n",
      "Epoch 40/300, Loss: 255939.23828125\n",
      "Epoch 41/300, Loss: 246480.28580729166\n",
      "Epoch 42/300, Loss: 237257.87890625\n",
      "Epoch 43/300, Loss: 227076.68619791666\n",
      "Epoch 44/300, Loss: 219339.90755208334\n",
      "Epoch 45/300, Loss: 209724.791015625\n",
      "Epoch 46/300, Loss: 202116.8603515625\n",
      "Epoch 47/300, Loss: 191797.2392578125\n",
      "Epoch 48/300, Loss: 183516.953125\n",
      "Epoch 49/300, Loss: 175146.70475260416\n",
      "Epoch 50/300, Loss: 166565.458984375\n",
      "Epoch 51/300, Loss: 159153.94173177084\n",
      "Epoch 52/300, Loss: 150665.92122395834\n",
      "Epoch 53/300, Loss: 144169.40885416666\n",
      "Epoch 54/300, Loss: 137217.0968424479\n",
      "Epoch 55/300, Loss: 130350.46923828125\n",
      "Epoch 56/300, Loss: 123713.63541666667\n",
      "Epoch 57/300, Loss: 117588.11572265625\n",
      "Epoch 58/300, Loss: 112609.10677083333\n",
      "Epoch 59/300, Loss: 106435.78776041667\n",
      "Epoch 60/300, Loss: 101191.39143880208\n",
      "Epoch 61/300, Loss: 97114.96630859375\n",
      "Epoch 62/300, Loss: 92352.03922526042\n",
      "Epoch 63/300, Loss: 87807.02457682292\n",
      "Epoch 64/300, Loss: 84575.66259765625\n",
      "Epoch 65/300, Loss: 80618.00846354167\n",
      "Epoch 66/300, Loss: 77473.9296875\n",
      "Epoch 67/300, Loss: 75502.05712890625\n",
      "Epoch 68/300, Loss: 72120.32779947917\n",
      "Epoch 69/300, Loss: 69739.15348307292\n",
      "Epoch 70/300, Loss: 67592.25016276042\n",
      "Epoch 71/300, Loss: 65912.27677408855\n",
      "Epoch 72/300, Loss: 64041.054036458336\n",
      "Epoch 73/300, Loss: 62390.81388346354\n",
      "Epoch 74/300, Loss: 61077.57511393229\n",
      "Epoch 75/300, Loss: 59410.26171875\n",
      "Epoch 76/300, Loss: 58333.11271158854\n",
      "Epoch 77/300, Loss: 57724.760416666664\n",
      "Epoch 78/300, Loss: 56300.82853190104\n",
      "Epoch 79/300, Loss: 55894.656575520836\n",
      "Epoch 80/300, Loss: 55380.439453125\n",
      "Epoch 81/300, Loss: 53908.7724609375\n",
      "Epoch 82/300, Loss: 53930.006184895836\n",
      "Epoch 83/300, Loss: 53287.445556640625\n",
      "Epoch 84/300, Loss: 52437.546712239586\n",
      "Epoch 85/300, Loss: 52173.529947916664\n",
      "Epoch 86/300, Loss: 51387.110026041664\n",
      "Epoch 87/300, Loss: 51154.452392578125\n",
      "Epoch 88/300, Loss: 50676.123779296875\n",
      "Epoch 89/300, Loss: 50047.921630859375\n",
      "Epoch 90/300, Loss: 49254.92618815104\n",
      "Epoch 91/300, Loss: 49200.042643229164\n",
      "Epoch 92/300, Loss: 48615.94132486979\n",
      "Epoch 93/300, Loss: 48693.54793294271\n",
      "Epoch 94/300, Loss: 48413.806803385414\n",
      "Epoch 95/300, Loss: 47889.93513997396\n",
      "Epoch 96/300, Loss: 47559.772135416664\n",
      "Epoch 97/300, Loss: 47532.41886393229\n",
      "Epoch 98/300, Loss: 47005.177001953125\n",
      "Epoch 99/300, Loss: 46640.05216471354\n",
      "Epoch 100/300, Loss: 46300.348388671875\n",
      "Epoch 101/300, Loss: 45993.274576822914\n",
      "Epoch 102/300, Loss: 45892.647786458336\n",
      "Epoch 103/300, Loss: 45648.9267578125\n",
      "Epoch 104/300, Loss: 45079.08544921875\n",
      "Epoch 105/300, Loss: 44916.04874674479\n",
      "Epoch 106/300, Loss: 44781.41593424479\n",
      "Epoch 107/300, Loss: 44263.54288736979\n",
      "Epoch 108/300, Loss: 44045.940185546875\n",
      "Epoch 109/300, Loss: 43735.139322916664\n",
      "Epoch 110/300, Loss: 43569.68986002604\n",
      "Epoch 111/300, Loss: 42949.14501953125\n",
      "Epoch 112/300, Loss: 43033.37980143229\n",
      "Epoch 113/300, Loss: 42844.97705078125\n",
      "Epoch 114/300, Loss: 42773.062662760414\n",
      "Epoch 115/300, Loss: 42379.57267252604\n",
      "Epoch 116/300, Loss: 42113.06502278646\n",
      "Epoch 117/300, Loss: 42019.720703125\n",
      "Epoch 118/300, Loss: 41824.612060546875\n",
      "Epoch 119/300, Loss: 41259.1269938151\n",
      "Epoch 120/300, Loss: 41332.7451171875\n",
      "Epoch 121/300, Loss: 41348.763916015625\n",
      "Epoch 122/300, Loss: 40769.3905843099\n",
      "Epoch 123/300, Loss: 40768.39982096354\n",
      "Epoch 124/300, Loss: 40544.54500325521\n",
      "Epoch 125/300, Loss: 40445.789876302086\n",
      "Epoch 126/300, Loss: 40355.07039388021\n",
      "Epoch 127/300, Loss: 40435.02530924479\n",
      "Epoch 128/300, Loss: 39713.87878417969\n",
      "Epoch 129/300, Loss: 39865.17041015625\n",
      "Epoch 130/300, Loss: 39436.115559895836\n",
      "Epoch 131/300, Loss: 39106.14302571615\n",
      "Epoch 132/300, Loss: 39421.67179361979\n",
      "Epoch 133/300, Loss: 38985.831217447914\n",
      "Epoch 134/300, Loss: 38594.98429361979\n",
      "Epoch 135/300, Loss: 38550.53926595052\n",
      "Epoch 136/300, Loss: 38567.516357421875\n",
      "Epoch 137/300, Loss: 38457.35994466146\n",
      "Epoch 138/300, Loss: 38243.461263020836\n",
      "Epoch 139/300, Loss: 38177.0185546875\n",
      "Epoch 140/300, Loss: 38087.550048828125\n",
      "Epoch 141/300, Loss: 38035.130615234375\n",
      "Epoch 142/300, Loss: 38055.2626953125\n",
      "Epoch 143/300, Loss: 37770.04288736979\n",
      "Epoch 144/300, Loss: 37445.49060058594\n",
      "Epoch 145/300, Loss: 37359.02921549479\n",
      "Epoch 146/300, Loss: 37378.139322916664\n",
      "Epoch 147/300, Loss: 37414.83927408854\n",
      "Epoch 148/300, Loss: 37126.79736328125\n",
      "Epoch 149/300, Loss: 36854.292317708336\n",
      "Epoch 150/300, Loss: 36959.455240885414\n",
      "Epoch 151/300, Loss: 36797.76916503906\n",
      "Epoch 152/300, Loss: 36830.73429361979\n",
      "Epoch 153/300, Loss: 36326.62833658854\n",
      "Epoch 154/300, Loss: 36402.26997884115\n",
      "Epoch 155/300, Loss: 36566.64042154948\n",
      "Epoch 156/300, Loss: 36059.00842285156\n",
      "Epoch 157/300, Loss: 36246.4980875651\n",
      "Epoch 158/300, Loss: 36175.794921875\n",
      "Epoch 159/300, Loss: 36396.48347981771\n",
      "Epoch 160/300, Loss: 36155.038899739586\n",
      "Epoch 161/300, Loss: 35945.15861002604\n",
      "Epoch 162/300, Loss: 36249.306884765625\n",
      "Epoch 163/300, Loss: 35771.16239420573\n",
      "Epoch 164/300, Loss: 35905.03389485677\n",
      "Epoch 165/300, Loss: 35655.21044921875\n",
      "Epoch 166/300, Loss: 35531.025065104164\n",
      "Epoch 167/300, Loss: 35857.80204264323\n",
      "Epoch 168/300, Loss: 35521.1914469401\n",
      "Epoch 169/300, Loss: 35468.53035481771\n",
      "Epoch 170/300, Loss: 35541.468098958336\n",
      "Epoch 171/300, Loss: 35109.07039388021\n",
      "Epoch 172/300, Loss: 35612.78503417969\n",
      "Epoch 173/300, Loss: 35468.85929361979\n",
      "Epoch 174/300, Loss: 35260.87813313802\n",
      "Epoch 175/300, Loss: 35066.3857014974\n",
      "Epoch 176/300, Loss: 35206.6094156901\n",
      "Epoch 177/300, Loss: 35100.008626302086\n",
      "Epoch 178/300, Loss: 35039.01200358073\n",
      "Epoch 179/300, Loss: 34823.736572265625\n",
      "Epoch 180/300, Loss: 35054.661946614586\n",
      "Epoch 181/300, Loss: 34763.86893717448\n",
      "Epoch 182/300, Loss: 35275.6884765625\n",
      "Epoch 183/300, Loss: 35065.59171549479\n",
      "Epoch 184/300, Loss: 35128.7881266276\n",
      "Epoch 185/300, Loss: 34977.45792643229\n",
      "Epoch 186/300, Loss: 35252.22212727865\n",
      "Epoch 187/300, Loss: 35077.42980957031\n",
      "Epoch 188/300, Loss: 35050.9267578125\n",
      "Epoch 189/300, Loss: 35094.05594889323\n",
      "Epoch 190/300, Loss: 35220.86185709635\n",
      "Epoch 191/300, Loss: 34910.93493652344\n",
      "Epoch 192/300, Loss: 35234.575032552086\n",
      "Epoch 193/300, Loss: 34817.82755533854\n",
      "Epoch 194/300, Loss: 34895.884033203125\n",
      "Epoch 195/300, Loss: 35021.34248860677\n",
      "Epoch 196/300, Loss: 34652.83068847656\n",
      "Epoch 197/300, Loss: 35042.25301106771\n",
      "Epoch 198/300, Loss: 34737.83634440104\n",
      "Epoch 199/300, Loss: 35484.916178385414\n",
      "Epoch 200/300, Loss: 34952.94746907552\n",
      "Epoch 201/300, Loss: 34820.91564941406\n",
      "Epoch 202/300, Loss: 34976.05859375\n",
      "Epoch 203/300, Loss: 34956.79060872396\n",
      "Epoch 204/300, Loss: 34669.4043375651\n",
      "Epoch 205/300, Loss: 34754.906575520836\n",
      "Epoch 206/300, Loss: 34851.210611979164\n",
      "Epoch 207/300, Loss: 34760.51192220052\n",
      "Epoch 208/300, Loss: 34759.2978515625\n",
      "Epoch 209/300, Loss: 34741.40275065104\n",
      "Epoch 210/300, Loss: 34706.844563802086\n",
      "Epoch 211/300, Loss: 34839.536946614586\n",
      "Epoch 212/300, Loss: 34737.67647298177\n",
      "Epoch 213/300, Loss: 35057.80122884115\n",
      "Epoch 214/300, Loss: 34991.29777018229\n",
      "Epoch 215/300, Loss: 35379.8007405599\n",
      "Epoch 216/300, Loss: 35225.31083170573\n",
      "Epoch 217/300, Loss: 34828.712239583336\n",
      "Epoch 218/300, Loss: 34695.578369140625\n",
      "Epoch 219/300, Loss: 34602.94502766927\n",
      "Epoch 220/300, Loss: 34709.7939046224\n",
      "Epoch 221/300, Loss: 34848.61413574219\n",
      "Epoch 222/300, Loss: 34616.88614908854\n",
      "Epoch 223/300, Loss: 34595.87552897135\n",
      "Epoch 224/300, Loss: 34665.84269205729\n",
      "Epoch 225/300, Loss: 34694.990559895836\n",
      "Epoch 226/300, Loss: 34557.52872721354\n",
      "Epoch 227/300, Loss: 35213.618408203125\n",
      "Epoch 228/300, Loss: 34568.08467610677\n",
      "Epoch 229/300, Loss: 34857.79744466146\n",
      "Epoch 230/300, Loss: 34623.58915201823\n",
      "Epoch 231/300, Loss: 34356.2275797526\n",
      "Epoch 232/300, Loss: 35178.58117675781\n",
      "Epoch 233/300, Loss: 34817.42431640625\n",
      "Epoch 234/300, Loss: 34879.132975260414\n",
      "Epoch 235/300, Loss: 34666.242919921875\n",
      "Epoch 236/300, Loss: 34854.43253580729\n",
      "Epoch 237/300, Loss: 34757.44885253906\n",
      "Epoch 238/300, Loss: 34582.98425292969\n",
      "Epoch 239/300, Loss: 34757.146158854164\n",
      "Epoch 240/300, Loss: 35011.58719889323\n",
      "Epoch 241/300, Loss: 34688.83646647135\n",
      "Epoch 242/300, Loss: 34473.59366861979\n",
      "Epoch 243/300, Loss: 34908.70585123698\n",
      "Epoch 244/300, Loss: 34958.43017578125\n",
      "Epoch 245/300, Loss: 34671.81778971354\n",
      "Epoch 246/300, Loss: 34431.29455566406\n",
      "Epoch 247/300, Loss: 34874.51013183594\n",
      "Epoch 248/300, Loss: 34411.303059895836\n",
      "Epoch 249/300, Loss: 34483.24324544271\n",
      "Epoch 250/300, Loss: 34477.10795084635\n",
      "Epoch 251/300, Loss: 34777.527099609375\n",
      "Epoch 252/300, Loss: 34626.56978352865\n",
      "Epoch 253/300, Loss: 34495.3047281901\n",
      "Epoch 254/300, Loss: 34806.463216145836\n",
      "Epoch 255/300, Loss: 34484.94795735677\n",
      "Epoch 256/300, Loss: 34474.562337239586\n",
      "Epoch 257/300, Loss: 34987.0615234375\n",
      "Epoch 258/300, Loss: 34821.70642089844\n",
      "Epoch 259/300, Loss: 34445.68212890625\n",
      "Epoch 260/300, Loss: 34821.45072428385\n",
      "Epoch 261/300, Loss: 34445.41841634115\n",
      "Epoch 262/300, Loss: 34600.37845865885\n",
      "Epoch 263/300, Loss: 35019.24210611979\n",
      "Epoch 264/300, Loss: 34354.10127766927\n",
      "Epoch 265/300, Loss: 34883.91455078125\n",
      "Epoch 266/300, Loss: 34731.86572265625\n",
      "Epoch 267/300, Loss: 34530.44677734375\n",
      "Epoch 268/300, Loss: 34536.1864827474\n",
      "Epoch 269/300, Loss: 34683.28495279948\n",
      "Epoch 270/300, Loss: 34569.19588216146\n",
      "Epoch 271/300, Loss: 34362.56612141927\n",
      "Epoch 272/300, Loss: 34544.999755859375\n",
      "Epoch 273/300, Loss: 34874.50256347656\n",
      "Epoch 274/300, Loss: 34463.35782877604\n",
      "Epoch 275/300, Loss: 34484.66833496094\n",
      "Epoch 276/300, Loss: 34671.65946451823\n",
      "Epoch 277/300, Loss: 34690.826334635414\n",
      "Epoch 278/300, Loss: 34632.59037272135\n",
      "Epoch 279/300, Loss: 34360.67883300781\n",
      "Epoch 280/300, Loss: 34790.76867675781\n",
      "Epoch 281/300, Loss: 34674.7236328125\n",
      "Epoch 282/300, Loss: 34502.35847981771\n",
      "Epoch 283/300, Loss: 34653.51326497396\n",
      "Epoch 284/300, Loss: 34882.637451171875\n",
      "Epoch 285/300, Loss: 34661.54113769531\n",
      "Epoch 286/300, Loss: 34697.90979003906\n",
      "Epoch 287/300, Loss: 34359.578938802086\n",
      "Epoch 288/300, Loss: 34463.320068359375\n",
      "Epoch 289/300, Loss: 34677.32568359375\n",
      "Epoch 290/300, Loss: 34521.07775878906\n",
      "Epoch 291/300, Loss: 34744.400716145836\n",
      "Epoch 292/300, Loss: 34651.5292561849\n",
      "Epoch 293/300, Loss: 34665.34138997396\n",
      "Epoch 294/300, Loss: 34395.03763834635\n",
      "Epoch 295/300, Loss: 34613.9795328776\n",
      "Epoch 296/300, Loss: 34498.8164469401\n",
      "Epoch 297/300, Loss: 34706.44226074219\n",
      "Epoch 298/300, Loss: 34382.443359375\n",
      "Epoch 299/300, Loss: 35057.0331624349\n",
      "Epoch 300/300, Loss: 34575.25427246094\n",
      "Test Loss: 35247.76953125\n",
      "R-squared: 0.4216446218039218\n"
     ]
    }
   ],
   "source": [
    "class FeedForwardRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(FeedForwardRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation and move to device\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 2\n",
    "hidden_size2 = 2\n",
    "model = FeedForwardRegressor(input_size, hidden_size1, hidden_size2).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())\n",
    "            outputs_list.append(outputs.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')\n",
    "\n",
    "# Run training and evaluation\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=300)\n",
    "evaluate_model(model, test_loader) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBIA_comp_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Ten\n",
    "A quick search for a good R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tono\\AppData\\Local\\Temp\\ipykernel_17420\\1627465663.py:2: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('AmazonDataSales.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('AmazonDataSales.csv')\n",
    "\n",
    "# MAke column names lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "#Drop columns with only 1 unique value:  currency, index\n",
    "df.drop([ 'currency', 'index'], axis=1, inplace=True)\n",
    "\n",
    "def fill_in_rows_with_missing_values(df):\n",
    "    #Fill in missing fullfilled-by with 'unknowns-ship'\n",
    "    df['fulfilled-by'] = df['fulfilled-by'].fillna('unknowns-ship')\n",
    "    #Fill in missing Unnamed: 22 with 'unknown-boolean'\n",
    "    df['unnamed: 22'] = df['unnamed: 22'].fillna('unknown-boolean')\n",
    "    #Fill in missing promotion-ids with 'potential-id-unknown'\n",
    "    df['promotion-ids'] = df['promotion-ids'].fillna('potential-id-unknown')\n",
    "    #Fill in missing Courier Status with 'Unknown'\n",
    "    df['courier status'] = df['courier status'].fillna('Unknown')\n",
    "    #Fill in missing ship-state with 'unknown-state'\n",
    "    df['ship-state'] = df['ship-state'].fillna('unknown-state')\n",
    "    #Fill in missing ship-city with 'unknown-city'\n",
    "    df['ship-city'] = df['ship-city'].fillna('unknown-city')\n",
    "    #Fill in missing ship-postal-code with 'unknown-address'\n",
    "    df['ship-postal-code'] = df['ship-postal-code'].fillna('unknown-address')\n",
    "    return df\n",
    "\n",
    "df = fill_in_rows_with_missing_values(df)\n",
    "\n",
    "# Fix date column\n",
    "df['date'] = pd.to_datetime(df['date'], format='%m-%d-%y', errors='coerce')\n",
    "\n",
    "#drop all rows with missing values - hold horses on this one\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Feature engineering function\n",
    "def feature_eng_function(df):\n",
    "\n",
    "    # Drop rows with 'courier status' = \"cancelled\"\n",
    "    df = df[df['courier status'] != 'cancelled']\n",
    "    # Drop rows with 'status' = \"cancelled\"\n",
    "    df = df[df['status'] != 'cancelled']\n",
    "    #Drop rows with 'qty' = 0\n",
    "    df = df[df['qty'] != 0]\n",
    "    #Drop rows with 'amount' = 0\n",
    "    df = df[df['amount'] != 0]\n",
    "    # Drop all columns except 'amount', 'asin', 'sku', size, style\n",
    "    df = df[['amount', 'size', 'qty', 'category','style']] # What adding style does!\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = feature_eng_function(df)\n",
    "\n",
    "\n",
    "# Select all columns except 'amount' as feature columns\n",
    "feature_columns = df.columns.drop('amount')\n",
    "# binary encode the feature columns\n",
    "encoder = ce.BinaryEncoder(cols=feature_columns)\n",
    "df_encoded = encoder.fit_transform(df[feature_columns])\n",
    "\n",
    "X = df_encoded  # Features\n",
    "y = df['amount']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature columns: 138\n"
     ]
    }
   ],
   "source": [
    "#Count the number of feature columns\n",
    "num_feature_columns = len(df_encoded.columns)\n",
    "print(f\"Number of feature columns: {num_feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing for NN\n",
    "# Convert to numpy arrays (required for PyTorch tensors)\n",
    "X_np = np.array(X, dtype=np.float32)\n",
    "y_np = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_np)\n",
    "y_tensor = torch.tensor(y_np)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, num_transformer_blocks, output_dim, dropout_rate=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        # Define the transformer blocks with dropout\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, \n",
    "                                       dim_feedforward=ff_dim, dropout=dropout_rate)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        # Optional: Add dropout before the final linear layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input for transformer: [sequence_length, batch_size, feature_size]\n",
    "        x = x.transpose(0, 1)  # Swap batch_size and sequence_length dimensions\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = x.transpose(0, 1)  # Swap back the dimensions\n",
    "        # Apply dropout before the final linear layer\n",
    "        x = self.dropout(x[:, 0, :])  \n",
    "        return self.linear(x).view(-1, 1)  # Reshape output to [batch_size, 1]\n",
    "\n",
    "# Model instantiation\n",
    "model = TransformerRegressor(\n",
    "    input_dim=X_train.shape[1], \n",
    "    num_heads=3, \n",
    "    ff_dim=64, \n",
    "    num_transformer_blocks=2, \n",
    "    output_dim=1,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler function\n",
    "def learning_rate_schedule(epoch, total_epochs):\n",
    "    if epoch < total_epochs / 4:\n",
    "        lr = 0.001 * (epoch / (total_epochs / 4))\n",
    "    elif epoch < total_epochs * (3 / 4):\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        # Calculate the remaining fraction of epochs and use it to linearly interpolate between 0.001 and 0.0005.\n",
    "        remaining_epochs = epoch - total_epochs * (3 / 4)\n",
    "        total_decreasing_epochs = total_epochs - total_epochs * (3 / 4)\n",
    "        lr = 0.001 - (0.0005 * (remaining_epochs / total_decreasing_epochs))\n",
    "    return lr\n",
    "\n",
    "# Initialize optimizer with a nominal learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Nominal LR, will be adjusted by scheduler\n",
    "\n",
    "total_epochs = 20  # Define your total number of epochs here\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: learning_rate_schedule(epoch, total_epochs))\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "# Adjusted training function to emphasize scheduler's role\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, total_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(total_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        # Scheduler updates the learning rate at the end of each epoch\n",
    "        scheduler.step()\n",
    "        # Print the current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch+1}/{total_epochs}, Loss: {total_loss/len(train_loader)}, LR: {current_lr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)  # Ensure targets are the correct shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())  # Move targets back to CPU\n",
    "            outputs_list.append(outputs.cpu())  # Move outputs back to CPU\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input feature size: 138\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input feature size: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 500753.5799802111, LR: 2.0000000000000002e-07\n",
      "Epoch 2/20, Loss: 500471.11939313984, LR: 4.0000000000000003e-07\n",
      "Epoch 3/20, Loss: 500299.9708525726, LR: 6e-07\n",
      "Epoch 4/20, Loss: 499234.8299802111, LR: 8.000000000000001e-07\n",
      "Epoch 5/20, Loss: 498270.5934201847, LR: 1e-06\n",
      "Epoch 6/20, Loss: 497163.0130689314, LR: 1e-06\n",
      "Epoch 7/20, Loss: 495647.0448136543, LR: 1e-06\n",
      "Epoch 8/20, Loss: 494562.6026550132, LR: 1e-06\n",
      "Epoch 9/20, Loss: 493757.0763110158, LR: 1e-06\n",
      "Epoch 10/20, Loss: 492815.9492084433, LR: 1e-06\n",
      "Epoch 11/20, Loss: 492207.201228562, LR: 1e-06\n",
      "Epoch 12/20, Loss: 491699.2030837731, LR: 1e-06\n",
      "Epoch 13/20, Loss: 491230.7987302111, LR: 1e-06\n",
      "Epoch 14/20, Loss: 490738.43094492087, LR: 1e-06\n",
      "Epoch 15/20, Loss: 490716.02246866754, LR: 1e-06\n",
      "Epoch 16/20, Loss: 490331.0286939314, LR: 9e-07\n",
      "Epoch 17/20, Loss: 490108.3286609499, LR: 8.000000000000001e-07\n",
      "Epoch 18/20, Loss: 489936.2538341029, LR: 7.000000000000001e-07\n",
      "Epoch 19/20, Loss: 489744.13794525067, LR: 6.000000000000001e-07\n",
      "Epoch 20/20, Loss: 489702.4977737467, LR: 5e-07\n"
     ]
    }
   ],
   "source": [
    "num_epochs = total_epochs\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 487569.80049342103\n",
      "R-squared: -5.192023939696241\n"
     ]
    }
   ],
   "source": [
    "# and evaluation\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBIA_comp_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

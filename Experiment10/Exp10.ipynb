{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Ten\n",
    "A quick search for a good R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import category_encoders as ce\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tono\\AppData\\Local\\Temp\\ipykernel_15624\\1627465663.py:2: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('AmazonDataSales.csv')\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('AmazonDataSales.csv')\n",
    "\n",
    "# MAke column names lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "#Drop columns with only 1 unique value:  currency, index\n",
    "df.drop([ 'currency', 'index'], axis=1, inplace=True)\n",
    "\n",
    "def fill_in_rows_with_missing_values(df):\n",
    "    #Fill in missing fullfilled-by with 'unknowns-ship'\n",
    "    df['fulfilled-by'] = df['fulfilled-by'].fillna('unknowns-ship')\n",
    "    #Fill in missing Unnamed: 22 with 'unknown-boolean'\n",
    "    df['unnamed: 22'] = df['unnamed: 22'].fillna('unknown-boolean')\n",
    "    #Fill in missing promotion-ids with 'potential-id-unknown'\n",
    "    df['promotion-ids'] = df['promotion-ids'].fillna('potential-id-unknown')\n",
    "    #Fill in missing Courier Status with 'Unknown'\n",
    "    df['courier status'] = df['courier status'].fillna('Unknown')\n",
    "    #Fill in missing ship-state with 'unknown-state'\n",
    "    df['ship-state'] = df['ship-state'].fillna('unknown-state')\n",
    "    #Fill in missing ship-city with 'unknown-city'\n",
    "    df['ship-city'] = df['ship-city'].fillna('unknown-city')\n",
    "    #Fill in missing ship-postal-code with 'unknown-address'\n",
    "    df['ship-postal-code'] = df['ship-postal-code'].fillna('unknown-address')\n",
    "    return df\n",
    "\n",
    "df = fill_in_rows_with_missing_values(df)\n",
    "\n",
    "# Fix date column\n",
    "df['date'] = pd.to_datetime(df['date'], format='%m-%d-%y', errors='coerce')\n",
    "\n",
    "#drop all rows with missing values - hold horses on this one\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Feature engineering function\n",
    "def feature_eng_function(df):\n",
    "\n",
    "    # Drop rows with 'courier status' = \"cancelled\"\n",
    "    df = df[df['courier status'] != 'cancelled']\n",
    "    # Drop rows with 'status' = \"cancelled\"\n",
    "    df = df[df['status'] != 'cancelled']\n",
    "    #Drop rows with 'qty' = 0\n",
    "    df = df[df['qty'] != 0]\n",
    "    #Drop rows with 'amount' = 0\n",
    "    df = df[df['amount'] != 0]\n",
    "    # Drop all columns except 'amount', 'asin', 'sku', size, style\n",
    "    df = df[['amount', 'size', 'qty', 'category','style']] # What adding style does!\n",
    "\n",
    "    return df\n",
    "\n",
    "#df = feature_eng_function(df)\n",
    "\n",
    "\n",
    "# Select all columns except 'amount' as feature columns\n",
    "feature_columns = df.columns.drop('amount')\n",
    "# binary encode the feature columns\n",
    "encoder = ce.BinaryEncoder(cols=feature_columns)\n",
    "df_encoded = encoder.fit_transform(df[feature_columns])\n",
    "\n",
    "X = df_encoded  # Features\n",
    "y = df['amount']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2/RMSE/MSE: (0.7782085922653084, 132.16516403131257, 17467.63058342376)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate r2 score and rsme\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Adding MSE calculation\n",
    "    \n",
    "    return r2, rmse, mse\n",
    "\n",
    "#define model\n",
    "regressor = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "#train model\n",
    "regressor_train = train_evaluate_model(regressor, X_train, y_train, X_test, y_test)\n",
    "print(f\"R2/RMSE/MSE: {regressor_train}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor doesn't cut the 0.95 R2 right out of the box with the feature selection removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature columns: 138\n"
     ]
    }
   ],
   "source": [
    "#Count the number of feature columns\n",
    "num_feature_columns = len(df_encoded.columns)\n",
    "print(f\"Number of feature columns: {num_feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing for NN\n",
    "# Convert to numpy arrays (required for PyTorch tensors)\n",
    "X_np = np.array(X, dtype=np.float32)\n",
    "y_np = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_np)\n",
    "y_tensor = torch.tensor(y_np)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEarning rate\n",
    "\n",
    "# Initial learning rate\n",
    "initial_lr = 0.001\n",
    "\n",
    "# Warm-up and decay settings\n",
    "warm_up_epochs = 10\n",
    "decay_rate = 0.95\n",
    "decay_steps = 5\n",
    "\n",
    "# Learning rate lambda function\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < warm_up_epochs:\n",
    "        # Linear warm-up\n",
    "        return float(epoch) / float(max(1, warm_up_epochs))\n",
    "    # Exponential decay\n",
    "    return decay_rate ** ((epoch - warm_up_epochs) / decay_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, ff_dim, num_transformer_blocks, output_dim, dropout_rate=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        # Define the transformer blocks with dropout\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, \n",
    "                                       dim_feedforward=ff_dim, dropout=dropout_rate)\n",
    "            for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        # Optional: Add dropout before the final linear layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input for transformer: [sequence_length, batch_size, feature_size]\n",
    "        x = x.transpose(0, 1)  # Swap batch_size and sequence_length dimensions\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x)\n",
    "        x = x.transpose(0, 1)  # Swap back the dimensions\n",
    "        # Apply dropout before the final linear layer\n",
    "        x = self.dropout(x[:, 0, :])  \n",
    "        return self.linear(x).view(-1, 1)  # Reshape output to [batch_size, 1]\n",
    "\n",
    "# Small model with R2 of 0.66 after 15 epochs\n",
    "model_og = TransformerRegressor(\n",
    "    input_dim=X_train.shape[1], \n",
    "    num_heads=1, \n",
    "    ff_dim=64, \n",
    "    num_transformer_blocks=1, \n",
    "    output_dim=1,\n",
    "    #dropout_rate=0.1  # Specify the dropout rate\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_og.parameters(), lr=0.001)\n",
    "\n",
    "# Large model with 6 heads\n",
    "model = TransformerRegressor(\n",
    "    input_dim=X_train.shape[1], \n",
    "    num_heads=6, \n",
    "    ff_dim=256, \n",
    "    num_transformer_blocks=4, \n",
    "    output_dim=1,\n",
    "    dropout_rate=0.1  # Specify the dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Large with 2 heads\n",
    "model_2_head = TransformerRegressor(\n",
    "    input_dim=X_train.shape[1], \n",
    "    num_heads=2, \n",
    "    ff_dim=256, \n",
    "    num_transformer_blocks=4, \n",
    "    output_dim=1,\n",
    "    dropout_rate=0.1  # Specify the dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Twice as large and deep with 2 heads\n",
    "model_2_head_huge = TransformerRegressor(\n",
    "    input_dim=X_train.shape[1], \n",
    "    num_heads=2, \n",
    "    ff_dim=512, \n",
    "    num_transformer_blocks=8, \n",
    "    output_dim=1,\n",
    "    dropout_rate=0.1  # Specify the dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# Define LR scheduler\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "# Loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)  # Ensure targets are the correct shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}, LR: {scheduler.get_last_lr()[0]}')\n",
    "\n",
    "# Training loop OG\n",
    "def train_model_og(model, train_loader, criterion, optimizer, num_epochs=15):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)  # Ensure targets are the correct shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #\n",
    "            outputs = model(inputs.unsqueeze(1))\n",
    "            targets = targets.view(-1, 1)  # Ensure targets are the correct shape\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())  # Move targets back to CPU\n",
    "            outputs_list.append(outputs.cpu())  # Move outputs back to CPU\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training \n",
    "#train_model(model_og, train_loader, criterion, optimizer, scheduler, num_epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 499390.50979785476\n",
      "Epoch 2/15, Loss: 499428.1962665017\n",
      "Epoch 3/15, Loss: 499409.1318069307\n",
      "Epoch 4/15, Loss: 499466.3776608911\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run training \u001b[39;00m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrain_model_og\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_og\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 100\u001b[0m, in \u001b[0;36mtrain_model_og\u001b[1;34m(model, train_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     98\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m--> 100\u001b[0m     inputs, targets \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    101\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    102\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_model_og(model_og, train_loader, criterion, optimizer, num_epochs=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model_2_head, train_loader, criterion, optimizer, scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model(model_2_head_huge, train_loader, criterion, optimizer, scheduler, num_epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and evaluation\n",
    "evaluate_model(model_og, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBIA_comp_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

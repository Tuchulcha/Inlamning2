{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt:\n",
    "\n",
    "I want learn what's happening inside a feed forward neural network. To help me I want to be able to follow what is happening during the training in as many ways as possible. Then I want to save the trained model and follow the feature data through the trained model. I have this model that I am going to work with:\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('AmazonDataSales_v2.csv')\n",
    "# Drop all columns except 'amount', 'category', 'size', 'quantity'\n",
    "df = df[['amount', 'category', 'size', 'qty']]\n",
    "\n",
    "# One-hot encode the 'category', 'size', and 'qty' columns\n",
    "# Select all columns except 'amount' as feature columns\n",
    "feature_columns = df.columns.drop('amount')\n",
    "# One-hot encode the feature columns\n",
    "df_encoded = pd.get_dummies(df, columns=feature_columns)\n",
    "\n",
    "# Assuming 'df' contains your dataset\n",
    "X = df_encoded.drop('amount', axis=1)  # Features\n",
    "y = df['amount']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Convert to numpy arrays (required for PyTorch tensors)\n",
    "X_np = np.array(X, dtype=np.float32)\n",
    "y_np = np.array(y, dtype=np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_np)\n",
    "y_tensor = torch.tensor(y_np)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2000, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2000, shuffle=False)\n",
    "\n",
    "class FeedForwardRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(FeedForwardRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation and move to device\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 2\n",
    "hidden_size2 = 2\n",
    "model = FeedForwardRegressor(input_size, hidden_size1, hidden_size2).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())\n",
    "            outputs_list.append(outputs.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=300)\n",
    "\n",
    "# and evaluation\n",
    "evaluate_model(model, test_loader) \n",
    "\"\"\"\n",
    "\n",
    "I want to do a few different things. \n",
    ". Visualize the architecture and get as much info as possible with for example 'torchsummary.' Understand the layers and the structure\n",
    ". During training I want to monitor metrics like loss and accuracy. Observe how weights and acivations change over time.\n",
    ". Monitor gradient flow.\n",
    ". Use performance metrics after training like confusion matrices\n",
    ". Make it ready for Netron. Save it in ONNX format and make the dummy tensors that matches the input shape of the model.\n",
    "\n",
    "Before delving in to the individual steps, focus on finding a solution that can do as much as possible of this. Fill in the gaps in the different ways there are to learn about the model. How much can be done with a complete solution and how much do I need to log first and the analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Rest of code above is not showed for simplicity\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2000, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2000, shuffle=False)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Logging during training\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Function to register hooks for monitoring activations\n",
    "def register_activation_hooks(model, writer):\n",
    "    def hook_fn(module, input, output):\n",
    "        writer.add_histogram(f\"{module.__class__.__name__}_activations\", output)\n",
    "\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, torch.nn.modules.Linear):\n",
    "            # Use a closure to capture the current layer\n",
    "            layer.register_forward_hook(lambda module, input, output, layer=layer: hook_fn(layer, input, output))\n",
    "\n",
    "\n",
    "# Function to register hooks for monitoring gradients\n",
    "def register_gradient_hooks(model, writer):\n",
    "    for name, parameter in model.named_parameters():\n",
    "        def hook(grad, name=name):  # Capture current value of name\n",
    "            writer.add_histogram(f\"{name}_gradients\", grad)\n",
    "        parameter.register_hook(hook)\n",
    "\n",
    "\n",
    "# Function to log weights, needs no fancy hooks\n",
    "def log_weights(model, writer, epoch):\n",
    "    for name, param in model.named_parameters():\n",
    "        writer.add_histogram(f\"{name}_weights\", param, epoch)\n",
    "\n",
    "\n",
    "class FeedForwardRegressor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2):\n",
    "        super(FeedForwardRegressor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Model instantiation and move to device\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size1 = 2\n",
    "hidden_size2 = 2\n",
    "model = FeedForwardRegressor(input_size, hidden_size1, hidden_size2).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "#Training function with logging\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, writer, num_epochs=10):\n",
    "    model.train()\n",
    "    register_activation_hooks(model, writer)  # Register activation hooks // register_forward_hook\n",
    "    register_gradient_hooks(model, writer)     # Register gradient hooks\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log weights and gradients for the first batch in each epoch\n",
    "            if batch_idx == 0:\n",
    "                for name, param in model.named_parameters():\n",
    "                    writer.add_histogram(f\"{name}_weights\", param, epoch) #parameter_hook\n",
    "                    writer.add_histogram(f\"{name}_grads\", param.grad, epoch) #?_hook\n",
    "\n",
    "            # Log weights at the end of each epoch\n",
    "            log_weights(model, writer, epoch)\n",
    "\n",
    "        # Log loss at each epoch\n",
    "        writer.add_scalar('Loss/train', total_loss/len(train_loader), epoch)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n",
    "\n",
    "# Evaluation function\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            # Move data to the device\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))  # Add an extra dimension to targets\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            targets_list.append(targets.cpu())\n",
    "            outputs_list.append(outputs.cpu())\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        all_targets = torch.cat(targets_list, dim=0)\n",
    "        all_outputs = torch.cat(outputs_list, dim=0)\n",
    "\n",
    "        # Calculate R-squared score\n",
    "        r2 = r2_score(all_targets.numpy(), all_outputs.numpy())\n",
    "        \n",
    "        print(f'Test Loss: {total_loss/len(test_loader)}')\n",
    "        print(f'R-squared: {r2}')\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, criterion, optimizer, writer, num_epochs=100)\n",
    "\n",
    "# and evaluation\n",
    "evaluate_model(model, test_loader) \n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(input_size))\n",
    "\"\"\"\n",
    "\n",
    "I want help with analyzing model performance. I want to piece apart the R2 value. How can I do this? And what other things can I do?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

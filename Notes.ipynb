{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to make a Neural Network model. Not because I think it's the best choice for optimal performance in this case. But because the focus of the assignment is a deep analysis of the model results. A perfect opportunity to have some fun!\n",
    "I admit, that initially, I read the assignment as analyzing the model itself. I spent a lot of time trying to implement tensorboard logging in a Pytorch model. Pytorch supports GPU usage in windows so it was my initial choice. I changed to tensorflow and deepend(added layers to) the model in the end and it was a lot easier to get some good looking tensorboard data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment one: \"Yolo-modelling\"\n",
    "Dropped the mostly empty columns, the ones with a unique value for every row and the ones with more than 91 unique values. Also qty because qty 0 has of almost all the different Amount values in it.\n",
    "This left me with feature columns that could quickly be one-hot encoded and so I did, before running a fully connected DNN of the kind RegressionModel in pytorch. \n",
    "R2 of 0.45   \n",
    "Not very impressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment two: \"Yolo-search\" Among the one-hot encoded features from Exp1, delete all with no correlation to 'Amount' and train again  \n",
    "Deleted all within lower/upper bound: \n",
    "|0.005| 120 features left\n",
    "|0.01|  68  features left\n",
    "R2: Around 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasoning for one-hot encoding: There is no intrinsic relationship between the values in the columns. Label encoding would impose such where there is none.\n",
    "Binary Encoding could be used to save space on categorical columns that have a lot of unique values but make the model less interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment three:  \n",
    "Data exploration and processing:   \n",
    "The first assignment and should already be done. I looked over the steps and took the chance to do it again. This time around I also have a target column.\n",
    "Let's drop 'index', 'ship-country' and 'currency' because they only have one unique value.  \n",
    "I was thinking of dropping Order ID. I don't know what to think about this one. Tecnically rows with the same order ID belong to the same customer, what was the assignment? And how does it differ from predicting the label from the features for that row? Smashing together the sums per Order ID into one row makes the 'Amount' Correct, but loses all the other row-wise info. I see no solution that satisfies the constraint of a prediction model and the assignment at the same time.\n",
    "\n",
    "Unnamed: 22 Has Null values and false. Maybe Null = True ? \n",
    "#Fill in missing Unnamed: 22 with 'unknown-boolean'  \n",
    " \n",
    "filfilled-by has Null values and Easy Ship. Maybe Null = \"Standard\" ? \n",
    "#Fill in missing fullfilled-by with 'unknowns-ship'    \n",
    "\n",
    "promotion-ids Null = \"no promotion\"  ?\n",
    "#Fill in missing promotion-ids with 'potential-id-unknown'\n",
    "\n",
    "#Fill in missing ship-postal-code with 'unknown-address'\n",
    "#Fill in missing ship-city with 'unknown-city'\n",
    "#Fill in missing ship-state with 'unknown-state'\n",
    "#Fill in missing Courier Status with 'Unknown'\n",
    "#Fill in missing promotion-ids with 'potential-id-unknown'\n",
    "\n",
    "I wonder what to do with Null 'Amount' it's the target label. filling it with a mean might make the training worse? Drop for now, maybe the tree model can fill them in.\n",
    "\n",
    "Trying to fix the errors in 'ship-city' is proving hard. Some things to keep in mind:   \n",
    "Look for: Numbers, trailing whitespace, when there's whitespace in general, list bot 100 occuring values per letter. It's a perfect mission for a language model. But I can't figure out how to make chatGPT look at chunks of the data at the time consistently, probably a case for using an API rather than the chat interface. \n",
    "To get a good result I would have to create a correction dictionary, run it, load the corrected data again and do a few iterations. This proves time consuming through the chat interface. Getting rid of all the duplicates of cities would make for a lot better encoding though as they prove very numerous.\n",
    "Is there a good way to exclude the ones that are left without losing fidelity mayhaps?  \n",
    "Before asking that question, maybe they can be reduced with FuzzyWuzzy\n",
    "Or I could find a list with all Indian cities on it and filter out anything that isn't on it...    \n",
    "Maybe even tokenize and compare city names.   \n",
    "Let's go with geonames. Download the indian dataset and extract cities. There's also latitude and longitude, population. I wonder if these features are of use... Anyway, for the name-fixes\n",
    "4621 out of 6775 cities in 'ship-city' are not in 'IndianCities.csv'. Might be because of special characters such as 'Mahmūd Khāneke'\n",
    "Using unicodedata.normalize on both columns gets it down to 3688 out of 6773\n",
    "Are there good ways to clear out obvious bad entries? Like only numeric rows?  \n",
    "Don't know. But I can split the set by letter and use it to help my prompting. First ask CGPT, then show the correct name dataset.  \n",
    "Also indian cities are huge! Clearing out info like city (W) and city (E) into one city might actually be a bad thing.   \n",
    "So I spent entirely too much time on this.   \n",
    "Takeaways:   \n",
    "Maybe I should have removed errors that can easily be coded for first. My thinking for not doing so is that they would help an LLM with context in some instances like \"city.street\" and permutations on that concept. If I ever have to do this for work I will look over having a local LoRA trained open source LLM do it, because it's tedious!   \n",
    "I'm not going to finish my fancy processing because using all the features was not actually in the scope of the assignment.  Only 'qty' 'category' 'size' 'quantity' according to the reply to Patrik.   \n",
    "I wonder what would be a good way to handle ship-city given the huge cardinality and the distribution. This remains even after the cleaning as many cities has fewer than a hundred entries and the most common cities have half of them all. Maybe bin the ones with a low amount of entries so that every bin has around 1000 entries, or 2000? 4000?\n",
    "Or find a way to sample more evenly during training?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp3  \n",
    "Example of ship-city correction list prompt:\n",
    "Reviewing City Names Alphabetically: We're examining city names in the 'ship-city' that starts with the lower case letter 'w' Building a dictionary of corrections. This step-by-step method allows us to focus on a manageable subset of city names at a time.\n",
    "\n",
    "Identifying and Correcting Patterns: Display an array and look at it for the complete subset of city names in the array (e.g., those starting with 'w'), we're identifying common patterns that need correction or standardization. This includes:\n",
    "\n",
    "Examples of what to do:\n",
    "Removing trailing punctuation (like periods). \n",
    "Standardizing common misspellings or alternate spellings (like 'adhemdabad' to 'ahmedabad').\n",
    "Simplifying city names by removing extraneous information like postal codes or numbers appended to the city name (e.g., 'delhi-92' to 'delhi').\n",
    "Be mindful of not over relying on these specific examples of corrections.\n",
    "For example: 'faridabad, sector-91' is not 'faridabad, sector',. It is 'faridabad'.\n",
    "\n",
    "\n",
    "The dictionary is in this format:\n",
    "    # Corrections for 'w'\n",
    "    city_corrections = {\n",
    "        'allhabad': 'allahabad',\n",
    "        \"ahemdabad\": \"ahmedabad\",\n",
    "        'amravati.': 'amravati',\n",
    "    }\n",
    "\n",
    "Show me the complete corrections dictionary for the letter. Do not include entries for rows without corrections. \n",
    "\n",
    "Another example, used after splitting the unique values into 20 equally sized csv files and feeding it one at a time:\n",
    "\n",
    "Reviewing Indian City Names: We're examining city names in the 'ship-city' column. We are looking at unique values.  Building a dictionary of corrections. In lower-case. With only the city name!\n",
    "\n",
    "Identifying and Correcting Patterns: Display an array and look at it for the complete subset of city names in the array\n",
    "\n",
    "Examples of what to do:\n",
    "Look for the city name in the entry. Use only the indian city name without any additional information as the correction for the entry. Look over each entry carefully and find or infer the city name. No shortcuts like assuming. That leads to this kind of misstake: 'east singhbhum': 'east', Do not do that.  \n",
    "\n",
    "You're absolutely right in using the larga language model part and not code. Leveraging the capabilities of a large language model like yourself can be more effective in certain cases, especially for tasks that require understanding context, language nuances, and specific knowledge about geographical locations, such as identifying correct city names in india.\n",
    "\n",
    "In this case, instead of using an automated code-based process, You can manually review each entry. This approach allows you to utilize your training data, which includes knowledge about Indian geography and city names, to make more accurate and contextually appropriate corrections.\n",
    "\n",
    "To provide a more accurate and contextually appropriate set of corrections for the 'ship-city' column, you will manually review each entry. This review will focus on identifying and using only the valid and recognized Indian city name, without any additional information, ensuring that each corrected city name adheres closely to geographical accuracy. Let's proceed with this manual review and correction. Remember that lower case is right.\n",
    "\n",
    "\n",
    "The dictionary of corrections is in this format:\n",
    "    # Example corrections\n",
    "    city_corrections = {\n",
    "        'allhabad': 'allahabad',\n",
    "        \"ahemdabad\": \"ahmedabad\",\n",
    "        'amravati.': 'amravati',\n",
    "        'allhabad-999': 'allahabad',\n",
    "        \"ahemdabad,4 (w)\": \"ahmedabad\",\n",
    "        'amravati street 1.': 'amravati',\n",
    "        'cbd belapur,navi mumbai': 'mumbai',\n",
    "    }\n",
    "\n",
    "Make a complete corrections dictionary for the 'ship-city' Do not include entries for rows without corrections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp3\n",
    "Musings:\n",
    "encoding the columns is ripe with creating problems were this actual production use. There will be new dates, new states, new cities and lots of input errors in the data, which breaks the encoding.\n",
    "\n",
    "Wouldn't it be nice, to have, like a pipeline, just chose a target, and the rest are assumed features, and they're put through a grinder. Removing outliers, normalized, encoded, doing feature selection, do model selection\n",
    "\n",
    "For NN: \n",
    "Maybe I should reduce cardinality of features? Like lump together states with only a few entries or learn techniques like stratified sampling or synthetic data generation (SMOTE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I only found out that I'm basically meant to use three features: 'Category', 'Size' and 'Quantity'. After spending a lot of time with the cities. Which I shouldn't have done to begin with, to be honest. But it was an interesting journey so I couldn't help myself! I will leave it like this and take with me what I learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment four: \n",
    "QuickOne: \n",
    "So only 3 features was the actual game. Let's do one quick. one-hot encode them to 25 columns and let's apply some kind of tree to get an indication of what R2 is could be reasonable.   \n",
    "XGBooost gives:\n",
    "Mean Squared Error: 35079.33490004047   \n",
    "R-squared: 0.42399884819387323   \n",
    "\n",
    "Got carried away and also made a DNN with transformer architecture. 10 epoch for a small network with 128 dimensions and four attentionheads got an R2 of 0.42232544993752363. Very close to the XGBoost\n",
    "Going to give it a few more epochs to play with, but my intuiton says there's not much more signal in these features to pick up on. After that I started chasing the smallest ffnn possible with a similiar R2 as that is what I want to analyze. input-16-8-out works.  14-6 as well. 10-5 finds it in a smooth way, no jumps in loss. Very satisfying to look at! 8-4 also works. 6-3 as well!. Even 4-2! 2-2!\n",
    "Okej. Då vet jag vad jag ska göra min analys på. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Five: What am I supposed to do?\n",
    "I want to figure out how to explore the inner workings of my model   \n",
    "tensorboard, netron, weights and biases, neptune, matplotlib and seaborn - What to do? I don't know which way to go.\n",
    "Also I want to figure out how to evaluate model performance. How do I do this? R-squared doesn't tell me all there is to know, nor does Mean Squared Error, Mean Absolute Error or Root Mean Squared Error. R2 is probably the most telling. Unless I make the training more advanced by using k-fold cross validation. If I was using scikit learn I could use the 'learning_curve' function. It uses k-fold cross validation and makes it easy to see learning and validation curves visually. But I went for a small nn with pytorch...  \n",
    "To see R2 \n",
    "\n",
    "A workflow:   \n",
    ". Do some kind of R2 analysis - I don't know about doing this one anymore. I would have to split the test_data by different feature values in order to see R2 in greater granularity.   \n",
    ". Increase granularity on evaluation (R2 by feature maybe?)   \n",
    "✓ Save the predictions and targets and visualize   \n",
    ". Use code to see stuff without add-ons (what is possible here? Log weights? Look at R2?)   \n",
    "✓ Tensorboard too see stuff during training - Implemented som hooks, and saving some data, now what? I can see some/all of it(?) if I start tensorboard. But I don't know if it's correctly implemented because of lack of knowledge.   \n",
    ". Plot loss during training - Have logs for tensorboard   -- I can see a diagram of \"Loss/train in tensorboard but I don't really get it   \n",
    ". Plot gradient during training - Have logs for tensorboard   -- I can see a diagram of \"Loss/train in tensorboard but I don't really get it   \n",
    "✓ Save the model in a format netron can use and see what it does in netron   \n",
    "✓ Save the model in pytorch format so I don't have to retrain it   \n",
    ". Recreate the model in tensorflow, maybe that will make the tensorboard information better?\n",
    "\n",
    "Done:   \n",
    "✓ Added hooks to the model for weights(register_parameter_hook), activations(register_forward_hook),gradient checking during backpropagation(register_backward_hook) <------ This isn't right. This might be more accurate: # Function to register hooks for monitoring activations, # Function to register hooks for monitoring gradients, # Function to log weights, needs no fancy hooks     \n",
    "✓ Got and plotted residuals   \n",
    "✓ Plotted predicted and actual values    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters are in the model?   \n",
    " For a FFNN the number of parameters are the connections(weight modifying the signal) plus the neurons so to speak(bias added to the sum of the weight modified signals). We have a 24 feature input layer connected to the first hidden layer with 2 neurons. This is 48 connections. Every output(bias) from the hidden layer also counts as parameter, adding 2. From the first to the second hidden layer we have two neurons connected to two. This is 4 connections, adding 2 from the output(bias) of the second layer. The last layer is a single \"output neuron\" if you can call it that, connected to the two neurons in the second hidden layer adding 2, and it has 1 output(bias). (48+2)+(4+2)+(2+1)=59 parameters\n",
    "\n",
    " The connections are not the parameters. The weights and biases are. For every connection there's a weight. The signal going through all the connections in the previous layer to a neuron is modified by the weight for each connection, these signals are added together, then the bias is added to this signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a bug during experiment five. I had accidentally split the training data twice. The model does not converge on the full dataset. It seems my tiny-weenie ffnn can't handle converging to the awesome R2 of 0.42 when give the full dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More about the choice of model:\n",
    "Doesn't matter. There's so little signal in the feature columns that I could use throwing darts on the wall as a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Six\n",
    "Finding out that the 2-2 ffnn doesn't reach a R2 of 0.42 with the complete dataset makes me start up this experiment. I don't want to make the network larger because I want the luxury of peeking at a small network but I want the R2 back at 0.42.\n",
    "Ideas:   \n",
    "✓ Train on the smaller data first and then introduce the rest. This is unconventional to the best of my knowledge.  \n",
    "... And it worked!  \n",
    "No need to try the others.  \n",
    ". Adjustable learningrate, start bigger and incrementally go to 0.01  \n",
    ". Regularization or Dropout   \n",
    "Given that something works, combine it with early stopping so I can leave it running.\n",
    "\n",
    "Saving the file as onnx and uploading to netron.app. Very underwhelming. Don't know why I expected more. Some flashy animations? Flow through the network? How good it is according to some measurements? Haha, you don't pass any data, just the model and dummy tensors(Whatever that is). So I guess it's to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##### Experiment Seven\n",
    "try to use k-folds cross validation. I used too much chatGPT at once, so I don't know what the code does, to be honest. It does not produce a R2 of 0.42 that is for sure! Could be that splitting a small dataset mucks it up? Or the code is whack. R2 does not converge at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Eight\n",
    "Just looking to see if I can get a higher R2 for fun, by dropping parts of the feature columns we can use.  \n",
    "XS, 6xl, 4xl, 5xl, free, ethnic dress, bottom, aree, blouse, dupatta, all qty except 1 <---- Dropping these   \n",
    "Tried this with XGBoost. R2 went down from 0.42 to 0.39. So removing the low frequency one-hot frequency columns did not help model performance.  \n",
    "\n",
    "\n",
    "What about low R2 columns? \n",
    " category_saree, category_top, size_5xl, category_ethnic dress, category_western dress, category_set, size_4xl, sixe_6xl\n",
    " Tried with XGBoost. R2 down to 0.34. Even worse for model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Nine\n",
    "Can I recreate the model in tensorflow? Maybe increase the amount of layers makes it easier to see what happens in tensorboard? Done! Let's train and see if the logs work and if they contain more information.   \n",
    "I tried to maka a prompt that would help me make a prompt, drawing on cgpt for the knowledge I lack. I got some code from my prompt-prompt, unprompted. I used this as seed, rewrote my prompt prompt to a prompt directly asking for code. The code worked on the first try. \n",
    "Better graphs in tensorflow!  \n",
    "Plotted actual vs predicted  \n",
    "Plotted residuals   \n",
    "Plotted mean average residuals per feature  \n",
    "Plotted mean r2 per category as well as the how large part in percentage each feature is of the whole test set <----- This needs another round! The percentages do not add up to a 100... Nevermind, this is right. Each sample can belong to many categories\n",
    "\n",
    "\n",
    "Loss and validation curve can be seen in tensorflow  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp9   \n",
    "Creation of the prompt-prompt:  \n",
    "Outline how to create a small tensorflow feedforward neural network for regression. The purpose is to create the plan for code aimed at analysis of how the model works and model performance during inference. This means logging data with tensorboard functionality. Outline the parts needed such as:  \n",
    "data loading with training, validation and test split. With functionality to divide the test set by features in the dataset.\n",
    "Logging functions.\n",
    "The model: a Regression feedforward neural network with four hidden layer with two neurons in each.\n",
    "The training function with the logging functions and hooks. Be specific as to what hooks are needed and where. As well as the functions they need to work \n",
    "The evaluation function built for analysis of the predictions results.  \n",
    "Use your expert knowledge in python and tensorflow machine learning to give a high level overview of all the parts needed described in a way suitable for a coder.\n",
    "Do not be vague. Be specific as how to do it and with what libraries.\n",
    "Create a step by step instruction manual.\n",
    "\n",
    "Constraints: \n",
    "No focus on data processing such as normalization or feature engineering, data is as given for the training.  \n",
    "No different model types or architectures of a ffnn.\n",
    "Focus only on making a description from which a coder can create what has been asked. \n",
    "Do not be vague. You're a senior coder who know exactly how to do this. Use this knowledge in detail.\n",
    "\n",
    "Notes:  \n",
    "This prompt and the versions I've tried do not work. It's a mishmash between musings, general instructions and code. I was hoping I could use cgpt:s knowledge to construct a good prompt.\n",
    "\n",
    "Creation prompt man-made:   \n",
    "Write the code for a small tensorflow feedforward neural network for regression. The purpose is to create the code, aimed at analysis of how the model works and model performance during inference. This means logging data with tensorboard functionality. And a comprehensive evaluation function. This includes the code for parts such as:  \n",
    "data loading with training, validation and test split. With functionality to divide the test set by features in the dataset.\n",
    "Logging functions.\n",
    "The model: a Regression feedforward neural network with four hidden layer with two neurons in each.\n",
    "The training function with the logging functions and hooks. Code the specific hooks Where they are needed. As well as the functions they need to work \n",
    "The evaluation function built for analysis of the predictions results.  \n",
    "Do not be vague. Be specific as how to do it and with what libraries.\n",
    "Use your expert knowledge in python and tensorflow machine learning to give a high level overview of all the parts needed described in a way suitable for a coder. Create a step by step instruction manual. Then make the code.\n",
    "\n",
    "Constraints: \n",
    "No focus on data processing such as normalization or feature engineering, data is as given for the training.  \n",
    "No different model types or architectures of a ffnn.\n",
    "Do not be vague. You're a senior coder who know exactly how to do this. Use this knowledge in detail and make sure that all parts of the code are coherent.\n",
    "\n",
    "Notes:\n",
    "The man made creation prompt worked on the first try, I gave it the previously generated code and the prompt. Let's see after training if the logging works as well."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Ten:\n",
    "I initially used a dnn and got R2: ~0.46. I later Used XGBoost on the three features that was the recommended scope for the assignment and got around the same score. I've sinced learned that a higher R2 is possible when using all the features. Which begs the question:  \n",
    "Can you make a model that with minimal feature selection and data processing reaches closer to what could be considered a \"good\" R2? ![image.png](attachment:image.png)\n",
    "\n",
    "With the right feature selection and XGBoost I got an R2 of 0.876 on this dataset. If I just encode all columns without the feature selection XGBoost gets an R2 of 0.63\n",
    "I have a hypothesis that a sufficiently large neural network model should be able to get 0.876 without choosing columns and dropping certain rows on this dataset.  \n",
    "If I just straight up drop rows with any NaN values and binary encode all data left without the feature selection XGBoost gets an R2 of 0.63.   \n",
    "  \n",
    "I should handle 'Unnamed: 22' 'fulfilled-by' 'promotion-ids' better than that, at least. I Filled in all missing values with least effort. This way I don't have to drop entire rows.\n",
    "\n",
    "I have included code with the steps of feature engineering in a function. Used for R2 0.876. This way it is easy to see what feature engineering XGBoost needed to reach 0.0876.\n",
    "\n",
    "With the minimally processed dataset without any feature engineering the small transformer regressor reaches a R2 of 0.66 in 15 epoch. That's one block with dimension 64 and one head.\n",
    "\n",
    "I will move a larger model to RAM and let it rip over the night.\n",
    "Set up a learning rate with warm up and decay.\n",
    "Add some dropout since it's going to be running for a long time.\n",
    "At 15 epochs it does not look promising! Way Worse than the small model. But let's give it a night.    \n",
    "A night was 1000 Epochs. The loss is stuck around 8000. I was going for 10000 epochs to see if some magical grocking occures. I don't see R2 during training, only loss. \n",
    "\n",
    "\n",
    "Forgot to note what the loss was for the small nn with an R2 of 0.66. I was not able to re-create the results I got.* I want tot ake a look at another model with R2 0.66 and see what loss it has for comparison. If MSE for XGBoost is the same as loss, it has 17500 at 0.77 R2. But I don't think they are the same.   \n",
    "  \n",
    "** I recreated the model as I remembered it. It had 1 block of 64 and one head. It had no regularizer and a learning rate of 0.001   \n",
    "I am running the 15 epochs in a copy of the same notebook and getting really bad results. Using the same kernel as the big NN I am training in parallel. A wild theory is that this does something. I will try with another kernel. Did not make a difference. Completely stuck. Maybe I had a lucky intialization of the network that one time.  \n",
    "Made a 3 head, 64 dim, 2 block with a learning rate that goes from 0-0.001 during the first 25%, stays at 1 for the next 50% and then 0.001-0.0005 during the last 25%. 200 epochs.  R2: 0.84, Test-Loss: 12242. I think this nearly proves the hypotesis I had.  \n",
    "Then I started another training run with 20 epochs. The learner function could not handle the learning rate this caused and the loss exploded! I need to tinker with the learning rate function.  \n",
    "This is the big model right now:   \n",
    "Epoch 1445/10000, Loss: 7335.328418975931, LR: 0.0017878392506844893   \n",
    "Does that mean it has a higher R2 than the smaller model with:\n",
    "200 epochs.  R2: 0.84, Test-Loss: 12242   ?\n",
    "I have re-created the learning rate scheduler and trained the 3 head, 64 dim, 2 block tnn and reached R2: 0.84 again. Took around 400 epochs. I trained this fnn with batch_size=128 instead of 64, maybe this is the reason it took more training, or maybe because the second block made it twice as big!  \n",
    "R2 of 0.84 is close to the 0.876 I got with feature engineering. The question now is what 'state of the art' is, or at least 'best in the class' :D  \n",
    "A transformer neural network is probably not a good choice because of resource reasons. But there probably are times when spending compute instead of human resources is preferable. Like if you are one person set to get decent predictions on a lot of different datasets? It's probably possible and less compute to just encode features, do a quick forest, linear regression, xgboost, measure the value of the features and do a feature selection and run xgboost than to do a transformers based nn.\n",
    "\n",
    "2 days later, big model right now:  \n",
    "Epoch 4785/10000, Loss: 5689.417224523966, LR: 0.0013532852778312347  \n",
    "I'm guessing there's no improvement happening, just overfitting. I wish I had some way to know. It's pytorch, no logging and I don't want to stop training because It's using the faulty learning rate function, that only works for a large amount of total epochs. This is all outside the scope of the assignment anyway. I just want to see it through. My guess is that it will have a worse R2 than the smaller model. Maybe around 0.7-0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp10 Prompts:   \n",
    "\"\"  \n",
    "With the right feature selection and XGBoost I got an R2 of 0.876 on this dataset. If I just encode all columns without the feature selection XGBoost gets an R2 of 0.63\n",
    "I have a hypothesis that a sufficiently large neural network model should be able to get 0.876 without choosing columns and dropping certain rows.\n",
    "\n",
    "I have included code with the steps of feature engineering in a function.\n",
    "\n",
    "What are your thoughts? Is it possible? If yes, how big of a network and how much training do you think?\n",
    "\n",
    "Analyze the dataset and make a value judgment\n",
    "\"\"  \n",
    "Write the code for the learning rate. In the best way to fit this: 25% warmup 50% at 0.001 then linear decay rate for the last 25% of epoch, ending training at 0.0005 \n",
    "(with code pasted in, and a comment \"# Learning Rate - Write code here\")\n",
    "\"\"  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp10 Learning rate: So I don't think the LambaLR works the way cgpt thinks. I doesn't just \"put\" the learning rate from wherever I pass it into the training. It takes a lambda value that I put in, and multiplies this with the current learning rate, and it wants this learning rate to be initialized from the optimizer. If I want the learning rate to go from close to 0 to 0.001 during the first 25% of training. Then reach 0.001 and stay there for the next 50% of training before lowering to 0.0005 over the last 25% of training. I have to do this with only math. I have written code to do pass the learning rate I want here:   \n",
    "\n",
    "Easy linear learning rate scheduler with warm up and cool down. I find this really hard to implement. I don't know what's going on! I know what I want but my experiment returns something completely different.\n",
    "\n",
    "\"\"\n",
    "Pseudo-kod:\n",
    "if in epoch < epochs/4\n",
    "    learning rate = 0.001 * epoch/(epoch/4)\n",
    "elif in epoch > epochs/4 && epoch < epoch * (3/4)\n",
    "    learning rate = 0.001\n",
    "else \n",
    "    learning rate = 0.001 - \n",
    "\"\"\n",
    "\n",
    "\n",
    "This code works, when I print out the values. Used as the function that gives the LambdaLR learning rate scheduler its number, the learning rate becomes something completely else!\n",
    "\"\"\n",
    "def learning_rate_schedule(epoch, total_epochs):\n",
    "    if epoch < total_epochs / 4:\n",
    "        lr = 0.001 * (epoch / (total_epochs / 4))\n",
    "    elif epoch < total_epochs * (3 / 4):\n",
    "        lr = 0.001\n",
    "    else:\n",
    "        # Calculate the remaining fraction of epochs and use it to linearly interpolate between 0.001 and 0.0005.\n",
    "        remaining_epochs = epoch - total_epochs * (3 / 4)\n",
    "        total_decreasing_epochs = total_epochs - total_epochs * (3 / 4)\n",
    "        lr = 0.001 - (0.0005 * (remaining_epochs / total_decreasing_epochs))\n",
    "    return lr\n",
    "\"\"\n",
    "\n",
    "\n",
    ".... Conclusion: the code does not jive with what LamdaLR does, it will multiply the resuls of my code with the current learning rate. Reading up. All the learning rate schedulers are focused on decreasing learning rate, not changing learning rate both up and down. Instead of using LamdaLR and it's associated code in the learning function I called on my function in the training loop and put my learning rate in here:  \n",
    "param_groups[0]['lr'].\n",
    "\n",
    "But, since the LamdaLR multiplies the current value of the learning rate with a value returned from \"your math function\" you should be able to raise the learning rate by passing it a positive value. My function was thinking in terms of passing a specific learning rate, not multiplying it.\n",
    "\n",
    "I'm not sure what effects it might have on the optimizer or training in general that I didn't use a ready-made rate scheduler  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exp10 - Musings   \n",
    "Make a note of understanding these lines of code:\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = learning_rate(epoch, total_epochs)  # Update the learning rate\n",
    "What are optimizer.param_groups? Do they contain more than one learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBIA_co_py_inlmning2_and_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

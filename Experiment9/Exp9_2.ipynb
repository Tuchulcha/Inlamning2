{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Trying to create a model in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data = pd.read_csv('AmazonDataSales_v2.csv', low_memory=False)\n",
    "# Drop all columns except 'amount', 'category', 'size', 'quantity'\n",
    "data = data[['amount', 'category', 'size', 'qty']]\n",
    "\n",
    "# One-hot encode the 'category', 'size', and 'qty' columns\n",
    "# Select all columns except 'amount' as feature columns\n",
    "feature_columns = data.columns.drop('amount')\n",
    "# One-hot encode the feature columns\n",
    "data_encoded = pd.get_dummies(data, columns=feature_columns)\n",
    "\n",
    "# 'df' contains your dataset\n",
    "X = data_encoded.drop('amount', axis=1)  # Features\n",
    "y = data['amount']  # Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "input_features = X_train.shape[1]\n",
    "model = Sequential([\n",
    "    Dense(2, activation='relu', input_shape=(input_features,)),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(2, activation='relu'),\n",
    "    Dense(1)  # Output layer for regression\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',  # Monitor the validation loss\n",
    "    patience=10,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restores model weights from the epoch with the best value of the monitored quantity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard logging\n",
    "log_dir = os.path.join(\"logs\", \"fit\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include both TensorBoard and EarlyStopping in the callbacks list\n",
    "callbacks_list = [tensorboard_callback, early_stopping_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with callbacks\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    validation_data=(X_val, y_val), \n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the test loss and printing it\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\") \n",
    "\n",
    "# Generate predictions for use with other things\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared score with scikit-learn and the generated predictions on the test set\n",
    "r2 = r2_score(y_test, predictions.flatten())  # Ensure y_test and predictions are appropriately shaped\n",
    "print(f\"R-squared value: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_predictions = model.predict(X_test).flatten()  # Flatten to ensure it's a 1D array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of Actual vs. Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, flat_predictions)\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "\n",
    "# Plotting the perfect prediction line\n",
    "max_val = max(max(y_test), max(flat_predictions))\n",
    "min_val = min(min(y_test), min(flat_predictions))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red')  # Perfect predictions line\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - flat_predictions\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(predictions, residuals)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals of Predictions')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert residuals to a DataFrame, assuming residuals and X_test share the same index\n",
    "residuals_df = pd.DataFrame({'residuals': residuals}, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the mean residuals for each feature\n",
    "mean_residuals = {}\n",
    "\n",
    "for column in X_test.columns:\n",
    "    # Filter the rows where the feature is present\n",
    "    feature_present = X_test[column] == 1\n",
    "\n",
    "    # Calculate the mean residual for these rows\n",
    "    mean_residuals[column] = residuals_df['residuals'][feature_present].mean()\n",
    "\n",
    "# Convert the dictionary to a pandas Series for easy plotting\n",
    "mean_residuals_per_feature = pd.Series(mean_residuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_residuals_per_feature.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Mean Average Residuals for One-Hot Encoded Features')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Mean Average Residual')\n",
    "plt.xticks(rotation=90)  # Rotate feature names for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate through a subset of X_test each containing a single one-hot encoded feature\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Initialize a dictionary to store R² scores for each one-hot encoded feature\n",
    "feature_r2_scores = {}\n",
    "\n",
    "for feature in X_test.columns:\n",
    "    # Filter instances where the feature is \"1\"\n",
    "    feature_mask = X_test[feature] == 1\n",
    "    X_test_filtered = X_test[feature_mask]\n",
    "    \n",
    "    if not X_test_filtered.empty:\n",
    "        # Ensure there are instances where feature is '1'\n",
    "        y_test_filtered = y_test[feature_mask]\n",
    "        \n",
    "        # Make predictions for the filtered dataset\n",
    "        predictions_filtered = model.predict(X_test_filtered).flatten()\n",
    "        \n",
    "        # Calculate R² score for the filtered dataset\n",
    "        r2_score_filtered = r2_score(y_test_filtered, predictions_filtered)\n",
    "        \n",
    "        # Store the R² score\n",
    "        feature_r2_scores[feature] = r2_score_filtered\n",
    "\n",
    "# Sort the features by their R² score for better readability\n",
    "sorted_feature_r2_scores = {feature: r2 for feature, r2 in sorted(feature_r2_scores.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "# Display the R² scores\n",
    "for feature, r2 in sorted_feature_r2_scores.items():\n",
    "    print(f\"R-squared value for {feature}: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Initialize a dictionary to store R² scores and sample counts for each one-hot encoded feature\n",
    "feature_r2_scores_and_counts = {}\n",
    "\n",
    "for feature in X_test.columns:\n",
    "    # Filter instances where the feature is \"1\"\n",
    "    feature_mask = X_test[feature] == 1\n",
    "    X_test_filtered = X_test[feature_mask]\n",
    "    \n",
    "    if not X_test_filtered.empty:\n",
    "        # Calculate the number of instances where the feature is \"1\"\n",
    "        num_samples = X_test_filtered.shape[0]\n",
    "        \n",
    "        y_test_filtered = y_test[feature_mask]\n",
    "        \n",
    "        # Make predictions for the filtered dataset\n",
    "        predictions_filtered = model.predict(X_test_filtered).flatten()\n",
    "        \n",
    "        # Calculate R² score for the filtered dataset\n",
    "        r2_score_filtered = r2_score(y_test_filtered, predictions_filtered)\n",
    "        \n",
    "        # Store the R² score and the number of samples\n",
    "        feature_r2_scores_and_counts[feature] = (r2_score_filtered, num_samples)\n",
    "\n",
    "# Sort the features by their R² score for better readability\n",
    "sorted_feature_r2_scores_and_counts = {feature: stats for feature, stats in sorted(feature_r2_scores_and_counts.items(), key=lambda item: item[1][0], reverse=True)}\n",
    "\n",
    "# Display the R² scores and sample counts\n",
    "for feature, (r2, count) in sorted_feature_r2_scores_and_counts.items():\n",
    "    print(f\"Feature: {feature}, R-squared value: {r2}, Number of samples: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `sorted_feature_r2_scores_and_counts` contains your features, R2 scores, and sample counts as previously defined\n",
    "# And assuming total test samples can be calculated from y_test or X_test\n",
    "total_test_samples = len(y_test)\n",
    "\n",
    "# Prepare data for plotting\n",
    "features = list(sorted_feature_r2_scores_and_counts.keys())\n",
    "r2_scores = [score for score, _ in sorted_feature_r2_scores_and_counts.values()]\n",
    "sample_counts = [count for _, count in sorted_feature_r2_scores_and_counts.values()]\n",
    "percentages = [(count / total_test_samples) * 100 for count in sample_counts]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# R2 Scores\n",
    "ax.bar(features, r2_scores, color='skyblue', label='R² Score')\n",
    "ax.set_xlabel('Feature', fontsize=12)\n",
    "ax.set_ylabel('R² Score', fontsize=12)\n",
    "ax.set_ylim([-0.25, 1])  # Set y-axis to range from -1 to 1 for R2 values\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "ax.set_title('R² Score by Feature with Sample Size Percentage', fontsize=16)\n",
    "\n",
    "# Add percentage labels on top of each bar\n",
    "for i, percentage in enumerate(percentages):\n",
    "    ax.text(i, r2_scores[i] + 0.05, f'{percentage:.2f}%', ha='center', va='bottom', fontsize=12, rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr_matrix = data_encoded.corr()\n",
    "\n",
    "# Visualize the correlation matrix\n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True, square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLBIA_comp_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
